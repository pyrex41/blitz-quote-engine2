This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: build_duckdb.py, async_csg.py, filter_utils.py, spot_check.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
async_csg.py
build_duckdb.py
filter_utils.py
spot_check.py

================================================================
Files
================================================================

================
File: async_csg.py
================
import httpx
from httpx import ReadTimeout

from toolz.functoolz import pipe
from datetime import datetime, timedelta
import configparser
from copy import copy
import asyncio
from babel.numbers import format_currency

from config import Config

from aiocache import cached

import time
from pprint import pprint
import csv
import logging
import random


def process_st(state_counties):
  out = set()
  for x in state_counties:
    xx = x.replace('ST ', 'ST. ').replace('SAINT ', 'ST. ').replace('SAINTE', 'STE.')
    out.add(xx)
  return out

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

from zips import zipHolder

# try to get current token

TIMEOUT = 60.0



lookup_dic = {}


def rate_limited(interval):

  def decorator(function):
    last_called = [0.0]

    def wrapper(*args, **kwargs):
      elapsed = time.time() - last_called[0]
      if elapsed >= interval:
        last_called[0] = time.time()
        fetch_sheet_and_export_to_csv()
      return function(*args, **kwargs)

    return wrapper

  return decorator


#fetch_sheet_and_export_to_csv()


def csv_to_dict(filename):
  with open(filename, 'r') as file:
    reader = csv.DictReader(file)
    result = {}
    for row in reader:
      # Convert 'Category' and 'ID' to integers
      row["Category"] = map_cat(row["Category"])  #int(row["Category"])
      # Check for null string key and filter it out
      if "" in row:
        del row[""]
      # Replace blank strings with None
      for key, value in row.items():
        if value == '':
          row[key] = None
      result[row["ID"]] = row
  return result


def map_cat(a_or_b: str):
  if a_or_b.lower() == "a":
    return 0
  elif a_or_b.lower() == "b":
    return 1
  else:
    return 2


class AsyncCSGRequest:

  def __init__(self, api_key):
    self.uri = 'https://csgapi.appspot.com/v1/'
    self.token_uri = "https://medicare-school-quote-tool.herokuapp.com/api/csg_token"
    self.api_key = api_key
    self.token = None  # Will be set asynchronously in an init method
    self.request_count = 0
    # Create shared client limits
    self.limits = httpx.Limits(max_keepalive_connections=50, max_connections=100)
    self._client = None  # Will be initialized later
    # Create error tracking for circuit breaker
    self.consecutive_errors = 0
    self.error_threshold = 10
    self.backoff_time = 1
    self.max_backoff = 30

  async def get_client(self):
    """Get a shared client with connection pooling to avoid too many open files."""
    if self._client is None:
      self._client = httpx.AsyncClient(timeout=TIMEOUT, limits=self.limits)
    return self._client

  async def close_client(self):
    """Close the shared client."""
    if self._client is not None:
      await self._client.aclose()
      self._client = None

  async def async_init(self):
    try:
      await self.set_token(await self.parse_token('token.txt'))
    except Exception as e:
      print(f"Could not parse token file: {e}")
      await self.set_token()

  async def parse_token(self, file_name):
    # Assuming the token file contains a section [token-config] with a token entry
    parser = configparser.ConfigParser()
    with open(file_name, 'r') as file:
      parser.read_file(file)
    return parser.get('token-config', 'token')

  async def set_token(self, token=None):
    self.token = token if token else await self.fetch_token()
    # Token is set, no need to write to a file unless it's a new token
    if not token:
      # Write the token to 'token.txt' asynchronously
      with open('token.txt', 'w') as f:
        f.write(f"[token-config]\ntoken={self.token}")

  async def fetch_token(self):
    client = await self.get_client()
    resp = await client.get(self.token_uri)
    if resp.status_code == 200:
      token = resp.json().get("csg_token")
      logging.info(f"Fetched_token is {token}")
      await self.set_token(token)
      return token
    else:
      return await self.fetch_token_fallback()

  async def fetch_token_fallback(self):
    ep = 'auth.json'
    values = {'api_key': self.api_key}
    client = await self.get_client()
    resp = await client.post(self.uri + ep, json=values)
    resp.raise_for_status()  # Will raise an exception for 4XX and 5XX status codes
    token = resp.json()['token']
    logging.warn(f"Reset token via csg: {token}")
    return token

  def GET_headers(self):
    return {'Content-Type': 'application/json', 'x-api-token': self.token}

  async def reset_token(self):
    print('Resetting token asynchronously')
    await self.set_token(token=None)

  async def get(self, uri, params, retry=5):
    """Get with improved error handling and exponential backoff."""
    for attempt in range(retry):
      try:
        # Apply exponential backoff if this is a retry
        if attempt > 0:
          backoff = min(self.backoff_time * (2 ** (attempt - 1)), self.max_backoff)
          logging.warning(f"Retry {attempt}/{retry} after {backoff}s backoff")
          await asyncio.sleep(backoff)
          
        client = await self.get_client()
        resp = await client.get(uri, params=params, headers=self.GET_headers())
        
        if resp.status_code == 403:
          await self.reset_token()
          resp = await client.get(uri, params=params, headers=self.GET_headers())
        elif resp.status_code in [429, 500, 502, 503, 504]:
          # Rate limited or server error - retry with backoff
          logging.warning(f"Received status code {resp.status_code}, retrying...")
          continue
          
        resp.raise_for_status()
        
        # Success - reset error counters
        self.consecutive_errors = 0
        self.backoff_time = 1
        
        self.request_count += 1
        return resp.json()
      except (httpx.ReadTimeout, httpx.ConnectTimeout, httpx.ReadError) as e:
        # Connection/timeout errors may indicate we're opening too many connections
        self.consecutive_errors += 1
        logging.warning(f"Connection error ({attempt+1}/{retry}): {str(e)}")
        
        # If we keep getting errors, close and recreate the client
        if self.consecutive_errors >= self.error_threshold:
          logging.warning("Too many consecutive errors, recreating client")
          await self.close_client()
          self.consecutive_errors = 0
          self.backoff_time = min(self.backoff_time * 2, self.max_backoff)
      except Exception as e:
        logging.error(f"API request error ({attempt+1}/{retry}): {str(e)}")
        self.consecutive_errors += 1
        
        # If this is the last attempt, re-raise
        if attempt == retry - 1:
          raise
    
    raise Exception(f"Request failed after {retry} attempts")

  async def _fetch_pdp(self, zip5):
    ep = 'medicare_advantage/quotes.json'
    payload = {
        'zip5': zip5,
        'plan': 'pdp',
    }
    resp = await self.get(self.uri + ep, params=payload)
    return resp

  async def fetch_pdp(self, zip5, *years):
    resp = await self._fetch_pdp(zip5)
    try:
      return self.format_pdp(resp, *years)
    except Exception as ee:
      emsg = {
          'Plan Name': "ERROR",
          'Plan Type': str(ee),
          'State': "CA",
          'rate': format_currency(0, 'USD', locale='en_US'),
          'year': list(years)[0]
      }
      return [emsg]

  def format_pdp(self, pdp_results, *_years):
    out = []
    years = list(_years)
    if len(years) == 0:
      years.append(datetime.today().year)
    for pdpr in pdp_results:
      dt_format = "%Y-%m-%dT%H:%M:%SZ"
      st_dt = pdpr['effective_date']
      dt = datetime.strptime(st_dt, dt_format)
      info = {
          'Plan Name': pdpr['plan_name'],
          'Plan Type': pdpr['plan_type'],
          'State': pdpr['state'],
          'rate': format_currency(pdpr['month_rate'] / 100,
                                  'USD',
                                  locale='en_US'),
          'year': dt.year
      }
      out.append(info)
    fout = filter(lambda x: x['year'] in years, out)
    return list(fout)

  async def fetch_quote(self, **kwargs):
    acceptable_args = [
        'zip5', 'county', 'age', 'gender', 'tobacco', 'plan', 'select',
        'effective_date', 'apply_discounts', 'apply_fees', 'offset', 'naic'
    ]
    payload = {}

    if 'retry' in kwargs:
      retry = kwargs.pop('retry')
    else:
      retry = 3

    for arg_name, val in kwargs.items():
      lowarg = arg_name.lower()
      if lowarg in acceptable_args:
        payload[lowarg] = val
    payload['apply_discounts'] = int(payload.get('apply_discounts', 0))

    ep = 'med_supp/quotes.json'
    resp = await self.get(self.uri + ep, params=payload, retry=retry)
    return resp

  async def fetch_advantage(self, **kwargs):
    acceptable_args = [
        'zip5', 'state', 'county', 'plan', 'offset', 'effective_date', 'sort',
        'order'
    ]
    payload = {}

    for arg_name, val in kwargs.items():
      lowarg = arg_name.lower()
      if lowarg in acceptable_args:
        payload[lowarg] = val

    if 'zip5' not in kwargs:
      raise ValueError("The 'zip5' argument is required.")

    ep = 'medicare_advantage/quotes.json'
    resp = await self.get(self.uri + ep, params=payload)
    return resp

  @rate_limited(3600)
  def format_rates(self, quotes, household):
    dic = {}
    for i, q in enumerate(quotes):
      rate = int(q['rate']['month'])
      naic = q['company_base']['naic']
      company_name = q['company_base']['name']
      plan = q['plan']

      if q['select']:
        k = company_name + ' // Select'
      else:
        k = company_name
      qq = q['rating_class']
      if qq:

        kk = k + ' // ' + q['rating_class']
      else:
        kk = k

      # workaround for those carriers in CSG that have multiple entries to handle discounts
      # may need something better if there's other reasons for multipe naic codes -- would require a rewrite
      arr = dic.get(naic, [])
      cat = 2
      disp = kk

      name_dict = csv_to_dict('cat.csv')

      ddic = name_dict.get(naic)
      if ddic:
        sub = False
        i = 1
        while i < 10:
          s = str(i)
          if ddic.get(s):
            sval = ddic[s]
            if sval.lower() in kk.lower():
              naic = f"{naic}00{s}"
              disp = f"{ddic.get('Name')} // {ddic.get(s, '').capitalize()}"
              cat = 1
              sub = True
              break
          i += 1
        if not sub:
          cat = ddic.get("Category", 2)
          disp = ddic.get("Name", kk)

      arr.append({
          "fullname": kk,
          "rate": rate,
          "naic": naic,
          "category": cat,
          "display": disp
      })
      dic[naic] = arr

    # continued workaround for carriers in CSG that don't handle household correctly
    d = []
    for a in dic.values():
      if len(a) == 1:  # this is the way it should work but CSG is pretty lame
        if bool(household):
          d = d + a
        else:
          # handling an edge case for Allstate where it returns a single "Rooommate" but doesn't put household in the fields
          a_filt = list(
              filter(lambda x: has_household(x) == bool(household), a))
          if len(a_filt) < len(a):
            d = d + a_filt
          else:
            d = d + a
      else:
        # what about the case(s) where len(2) but they actually aren't putting household in the fields? Trying to handle that here
        a_filt = list(filter(lambda x: has_household(x) == bool(household), a))
        if len(a_filt) < len(a):
          a_add = a_filt
        else:
          a_add = a

        a_add = sorted(a_add, key=lambda x: "//" in x["fullname"])
        if len(a_add) > 1:
          for i in range(1, len(a_add)):
            a_add[i]["category"] = 1  # category 1 for anything after the first

        d = d + a_add

    slist = sorted(d, key=lambda x: x["rate"])
    out_list = []
    for dic in slist:
      out_list.append({
          'company': dic["fullname"],
          'rate': dic["rate"] /
          100,  #format_currency(dic["rate"]/100, 'USD', locale='en_US'),
          'naic': dic["naic"],
          'plan': plan,
          'category': dic["category"],
          'display': dic["display"],
          'type': f'Supplemental',
      })
    return out_list

  def filter_quote(self,
                   quote_resp,
                   household=False,
                   custom_naic=None,
                   select=False):

    try:
      fresp = list(filter(lambda x: x['select'] == False,
                          quote_resp)) if not select else quote_resp
    except Exception as e:
      logging.error(f"Error in filter_quote: {str(e)}")
      raise

    if custom_naic:
      return pipe(
          list(
              filter(lambda x: int(x['company_base']['naic']) in custom_naic,
                     fresp)), self.format_rates)
    else:
      return self.format_rates(fresp, household=household)

  def format_results(self, results):
    logging.info(results)
    plan_dict = {}
    for r in results:
      for ol in r:
        plan = ol['plan']
        arr = plan_dict.get(plan, [])
        arr.append({
            'company': ol['company'],
            'rate': ol['rate'],
            'naic': ol['naic'],
            'category': ol['category'],
            'display': ol['display']
        })
        plan_dict[plan] = arr
    return plan_dict

  async def load_response_inner(self, query_data, delay=None):
    if delay:
      await asyncio.sleep(delay)
      print("Sleeiping ", delay)
    resp = await self.fetch_quote(**query_data, retry=4)
    return resp

  async def load_response_all_inner(self, query_data, delay=None):
    results = {}
    plans_ = query_data.pop('plan')
    tasks = []
    p_actual = []
    for p in ['A', 'B', 'C', 'D', 'F', 'G', 'HDF', 'HDG', 'K', 'L', 'M', 'N']:
      if p in plans_:
        p_actual.append(p)
    for i, p in enumerate(p_actual):
      qu = copy(query_data)
      qu['plan'] = p
      tasks.append(self.load_response_inner(qu, delay))

    for task in asyncio.as_completed(tasks):
      result = await task
      results.update(result)

    return results  # self.format_results(results)

  @cached(ttl=36_000)
  async def load_response_all(self, query_data, delay=None):
    return await self.load_response_all_inner(query_data, delay=delay)

  @cached(ttl=36_000)
  async def load_response(self, query_data, delay=None):
    return await self.load_response_inner(query_data, delay=delay)

  async def get_companies(self):
    uri = self.uri + "medicare_advantage/open/companies.json"
    resp = await self.get(uri, {})
    return resp

  async def calc_counties(self, state):
    zips = zipHolder('static/uszips.csv')
    state_zips = [k for (k, v) in zips.zip_states.items() if v == state]
    state_zip_county = []

    for z in state_zips:
      counties = zips.lookup_county(z) 

      try:
        for c in counties:
          if c != 'None':
            state_zip_county.append((z, c))
          else:
            logging.warn(f"County for {z} is {c}")
      except Exception as ee:
        logging.warn(ee)

    lookup_dict = {
        '20699': {},  # Ace / Chubb
        '72052': {},  # Aetna
        '79413': {},  # UHC
    }

    for tup in state_zip_county:
      z, county = tup
      dt = (datetime.now() +
            timedelta(days=32)).replace(day=1).strftime('%Y-%m-%d')
      for k in lookup_dict.keys():
        if z in lookup_dict[k].keys():
          continue
        else:
          logging.info(f"submitting query for zip: {z} - naic: {k}")
          r = await self.fetch_quote(zip5=int(z),
                                     county=county,
                                     age=65,
                                     gender="M",
                                     tobacco=0,
                                     effective_date=dt,
                                     naic=k,
                                     plan="N")
          zbase = r[0]['location_base']['zip5']
          logging.info(f"{len(zbase)} zips for {k}")
          zip_zero = next((z for z in zbase if z in state_zips), None)
          for i in zbase:
            lookup_dict[k][i] = zip_zero

    stats = {'total_zips': len(state_zips)}
    for k, v in lookup_dict.items():
      stats[k] = len(v)
      stats[f"{k}-unique"] = len(set(v))
    out = {}
    for kk, vdic in lookup_dict.items():
      outmap = {v: i for i, v in enumerate(list(set(vdic.values())))}
      d = {k: outmap[v] for k, v in vdic.items()}
      out[kk] = d

    return out, stats

  async def calc_counties2(self, state):
    zips = zipHolder('static/uszips.csv')
    state_zips = [k for (k, v) in zips.zip_states.items() if v == state]
    state_zip_county = []

    for z in state_zips:
      counties = zips.lookup_county(z)

      try:
        for c in counties:
          if c != 'None':
            state_zip_county.append((z, c))
          else:
            logging.warn(f"County for {z} is {c}")
      except Exception as ee:
        logging.warn(ee)
        
    #from lookup_index import lookup_dic
    lookup_list = { k[0]: [] for k in lookup_dic.get(state, [])}
    flat_list = { k[0]: [] for k in lookup_dic.get(state, [])}

    for tup in state_zip_county:
      z, county = tup
      dt = (datetime.now() +
            timedelta(days=32)).replace(day=1).strftime('%Y-%m-%d')
      for k in lookup_list:
        if z not in flat_list[k]:
          try:
            rr = await self.fetch_quote(zip5=int(z),
                                        county=county,
                                        age=65,
                                        gender="M",
                                        tobacco=0,
                                        effective_date=dt,
                                        naic=k,
                                        plan="N")
            #naic_filter = lambda x: x['company_base']['naic'] == k
            #rr = list(filter(naic_filter, rrr))
            if len(rr) > 0:
              x = rr[0]
              zbase = x['location_base']['zip5']
              kk = x['company_base']['naic']
              logging.info(f"{len(zbase)} zips for {kk}")
              if kk not in flat_list:
                flat_list[kk] = []
                lookup_list[kk] = []
              lookup_list[kk].append(zbase)
              flat_list[kk] = list(set(flat_list[kk] + zbase))
              logging.info(f"{len(flat_list[kk])} zips for {kk} -- {z}")
            else:
              logging.warn(f"No results for {z} - {county}")
          except Exception as ee:
            logging.warn(f"No results for {z} - {county} -- {ee}")
    for k in lookup_list:
      logging.info(f"{k} has {len(lookup_list[k])} zip regions")

    return lookup_list
    
  async def calc_naic_map_zip(self, state, naic, first_result=None):
    if state == 'WY' and naic == '82538': # workaround for weirdness
      logging.warn(f"{naic} skipped by workaround")
      return []
    zips = zipHolder('static/uszips.csv')
    state_zips = [k for (k, v) in zips.zip_states.items() if v == state]

    lookup_list = []
    processed_zips = set()

    zero_count = 0
    random.shuffle(state_zips)

    params = {
      "zip5": state_zips[0],
      "age": 65,
      "gender": "M",
      "tobacco": 0,
      "effective_date": (datetime.now() + timedelta(days=32)).replace(day=1).strftime('%Y-%m-%d'),
      "naic": naic,
    }
    if state not in ['MN', 'WI', 'MA', 'NY']:
      params["plan"] = "G"

    first_result = await self.fetch_quote(**params) if first_result is None else first_result 
    if len(first_result) == 0:
      logging.warn(f"No results for {naic} in {state}. The plan may not be offered in this state.")
      return []
    else:
      zbase = set(first_result[0]['location_base']['zip5'])
      logging.info(f"{len(zbase)} zips for {naic}")
      lookup_list.append(zbase)
      processed_zips.update(zbase)

    for z in state_zips:
      if z not in processed_zips:
        try:
          params['zip5'] = z
          rr = await self.fetch_quote(**params)
          if len(rr) > 0:
            x = rr[0]
            zbase = set(x['location_base']['zip5'])
            logging.info(f"{len(zbase)} zips for {naic}")
            
            # Check if zbase is already in lookup_list
            existing_index = next((i for i, existing_zbase in enumerate(lookup_list) if existing_zbase == zbase), None)
            if existing_index is not None:
                # Replace existing zbase
                zbase.add(z)
                lookup_list[existing_index] = zbase
            else:
                # Append new zbase
                zbase.add(z)
                lookup_list.append(zbase)

            processed_zips.update(zbase)

            if len(zbase) == 0:
              zero_count += 1
              if zero_count > 30:
                logging.warn(f"{naic} has {zero_count} zero regions -- exiting")
                return []
          else:
            logging.warn(f"No results for {z}")
        except Exception as ee:
          logging.warn(f"No results for {z} -- {ee}")

    logging.info(f"{naic} has {len(lookup_list)} zip regions")

    return lookup_list
  
  async def calc_humana_workaround(self, state_counties, sc_dict, processed_counties, params, zips, list_of_groups):
    group_extra = set()
    p_state_counties = process_st(state_counties)

    city_items = set()
    for county in p_state_counties:
      if county not in processed_counties:
        try:
          county_zip = next(z for z in sc_dict.get(county, []))
          logging.info(f"county_zip: {county_zip}")
          params['zip5'] = county_zip

          if county_zip is None:
            logging.warn(f"No 1:1 zip code found for county: {county}")
            zip_to_use = zips.lookup_zip(county)
            params['zip5'] = zip_to_use
            params['county'] = county
          elif 'county' in params:
            params.pop('county')
          
          rr = await self.fetch_quote(**params)
          
          if len(rr) > 0:
            x = rr[0]
            county_base_raw = set(x['location_base']['county'])
            county_base_raw = process_st(county_base_raw)

            for x in list(county_base_raw):
              if x.endswith(' CITY'): 
                city_items.add(x[:-5])
              else:
                x_in = [x in group for group in list_of_groups]
                if not any(x_in):
                  group_extra.add(x)
                processed_counties.add(x)
          else:
            logging.warn(f"No results for {county_zip} - {county}")
        except Exception as ee:
          logging.warn(f"Error processing {county}: {ee}")
    return list_of_groups + [group_extra]
  
  
  async def calc_naic_map_county(self, state, naic, first_result=None):
    if state == 'WY' and naic == '82538':  # workaround for weirdness
        logging.warn(f"{naic} skipped by workaround")
        return []

    zips = zipHolder('static/uszips.csv')
    state_zips = [k for (k, v) in zips.zip_states.items() if v == state]
    state_counties = set()

    single_county_zips = set()
    sc_dict = {}

    for z in state_zips:
        counties = zips.lookup_county2(z)
        state_counties.update(counties)
        if len(counties) == 1:
            single_county_zips.add(z)
            zz = sc_dict.get(counties[0], [])
            zz.append(z)
            sc_dict[counties[0]] = zz

    


    # Shuffle the single_county_zips list
    single_county_zips = list(single_county_zips)
    random.shuffle(single_county_zips)

    state_counties.discard('None')
    state_counties.discard(None)

    # Log counties that aren't keys in sc_dict
    counties_not_in_sc_dict = state_counties - set(sc_dict.keys())
    for county in counties_not_in_sc_dict:
        logging.warn(f"County not found in sc_dict: {county}; no 1:1 zip code found")

    processed_counties = set()
    processed_zips = set()

    zero_count = 0

    lookup_list = []

    params = {
      "zip5": single_county_zips[0],
      "age": 65,
      "gender": "M",
      "tobacco": 0,
      "effective_date": (datetime.now() + timedelta(days=32)).replace(day=1).strftime('%Y-%m-%d'),
      "naic": naic,
    }
    if state not in ['MN', 'WI', 'MA', 'NY']:
      params["plan"] = "G"

    first_result = await self.fetch_quote(**params) if first_result is None else first_result 
    
    city_items = set()
    if len(first_result) == 0:
      logging.warn(f"No results for {naic} in {state}. The plan may not be offered in this state.")
      return []
    elif len(first_result[0]['location_base']['county']) == 0:
      logging.warn(f"This state/naic does not support county mapping: {state}/{naic}")
      return []
    else:
      county_base_raw = set(first_result[0]['location_base']['county'])
      logging.info(f"{len(county_base_raw)} counties for {naic}")
      
      for x in list(county_base_raw):
        if x.endswith(' CITY'): # VA workaround
          city_items.add(x[:-5])
        if state == 'FL':
          if x == 'SAINT JOHNS': # FL workaround
            city_items.add(x)
            county_base_raw.add('ST. JOHNS')

      county_base = county_base_raw - city_items
      

      
      county_base = process_st(county_base_raw)

      lookup_list.append(county_base)
      processed_counties.update(county_base)
      logging.info(f"{len(processed_counties)} counties processed for {naic}")


    
      # workaround for HUMANA LA
      if state == 'LA' and naic in ['73288', '60984', '60052']:
        group1 = set([
          'JEFFERSON',
          'ORLEANS',
          'PLAQUEMINES',
          'ST. BERNARD',
          'ST. CHARLES',
          'ST. TAMMANY',
          'WASHINGTON',
        ])
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [group1])
      
      # workaround for HUMANA AL
      if state in ['AL', 'MD', 'AK'] and naic in ['73288', '60984', '60052', '88595', '60219']:
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [])
      
      if state == 'TX' and naic in ['73288', '60984', '60052', '88595', '60219']:
        group1 = set([
            'AUSTIN', 'BAILEY', 'BRAZORIA', 'CHAMBERS', 'COLORADO', 'FORT BEND', 'GALVESTON', 'HARDIN', 
            'HARRIS', 'JEFFERSON', 'LIBERTY', 'MATAGORDA', 'MONTGOMERY', 'ORANGE', 'SAN JACINTO', 'WALKER', 
            'WALLER', 'WASHINGTON', 'WHARTON'
        ])
        group2 = set([
            'ANDREWS', 'ARANSAS', 'BEE', 'BORDEN', 'BROOKS', 'CALHOUN', 'CAMP', 'CLAY', 'COLLIN', 'COMAL', 
            'COOKE', 'CRANE', 'DALLAS', 'DELTA', 'DENTON', 'DEWITT', 'DUVAL', 'ECTOR', 'ELLIS', 'FANNIN', 
            'FRANKLIN', 'GLASSCOCK', 'GRAYSON', 'GUADALUPE', 'HOPKINS', 'HOWARD', 'HUNT', 'JACKSON', 
            'JEFF DAVIS', 'JIM HOGG', 'JIM WELLS', 'KARNES', 'KAUFMAN', 'KENEDY', 'KLEBERG', 'LAMAR', 
            'LAVACA', 'LOVING', 'MARTIN', 'MIDLAND', 'MONTAGUE', 'NAVARRO', 'NUECES', 'PECOS', 'RAINS', 
            'RED RIVER', 'REEVES', 'REFUGIO', 'ROCKWALL', 'SAN PATRICIO', 'TITUS', 'UPTON', 'VAN ZANDT', 
            'VICTORIA', 'WARD', 'WILSON', 'WINKLER', 'WOOD'
        ])
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [group1, group2])

      if state == 'IL' and naic in ['73288', '60984', '60052', '88595', '60219']:
        group1 = set([
          'COOK', 'DEKALB', 'DUPAGE', 'GRUNDY', 'KANE', 'KENDALL', 'LASALLE', 'LAKE', 
          'LIVINGSTON', 'MCHENRY', 'WILL'
        ])
        group2 = set([
          'BOND', 'CALHOUN', 'CHAMPAIGN', 'CLINTON', 'FORD', 'GREENE', 'IROQUOIS', 
          'JERSEY', 'KANKAKEE', 'MACOUPIN', 'MADISON', 'MONROE', 'MONTGOMERY', 
          'PERRY', 'RANDOLPH', 'ST. CLAIR', 'WASHINGTON'
        ])
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [group1, group2])
      
      # workaround for HUMANA MO
      if state == 'MO' and naic in ['73288', '60984', '60052', '88595', '60219']:
        group1 = set([
            'ADAIR', 'BATES', 'CLAY', 'COLE', 'JACKSON', 'JEFFERSON', 'LINCOLN',
            'MARIES', 'OSAGE', 'PLATTE', 'RANDOLPH', 'RAY', 'ST. CHARLES',
            'ST. LOUIS', 'SCHUYLER',
        ])
        group2 = set([
            'ANDREW', 'AUDRAIN', 'BARTON', 'BOONE', 'BUCHANAN', 'CALDWELL',
            'CAMDEN', 'CASS', 'CLARK', 'CLINTON', 'DAVIESS', 'GENTRY',
            'HICKORY', 'JASPER', 'KNOX', 'LAFAYETTE', 'MACON', 'MARION',
            'MILLER', 'MONITEAU', 'NEWTON', 'PHELPS', 'PIKE', 'PULASKI',
            'PUTNAM', 'RALLS', 'ST. CLAIR', 'ST. FRANCOIS', 'STE. GENEVIEVE',
            'SCOTLAND', 'SULLIVAN', 'TANEY', 'WARREN', 'WASHINGTON',
        ])
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [group1, group2])
      # workaround for HUMANA FL
      if state == 'FL' and naic in ['73288', '60984', '60052', '88595', '60219']:
        group1 = set([
          'BROWARD',
          'MIAMI-DADE',
          'PALM BEACH',
        ])
        group2 = set([
          'BAKER',
          'BAY',
          'BREVARD',
          'CHARLOTTE',
          'CLAY',
          'COLLIER',
          'DUVAL',
          'HERNANDO',
          'HILLSBOROUGH',
          'INDIAN RIVER',
          'LAKE',
          'LEE',
          'MANATEE',
          'MARTIN',
          'NASSAU',
          'OKALOOSA',
          'ORANGE',
          'OSCEOLA',
          'PASCO',
          'PINELLAS',
          'ST. JOHNS',
          'ST. LUCIE',
          'SARASOTA',
          'SEMINOLE',
          'VOLUSIA'
        ])
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [group1, group2])
      
      if state == 'MI' and naic in ['73288', '60984', '60052', '88595', '60219']:
        return []
      if state == 'MI' and naic in ['60984']:
        group1 = set([
          'GRATIOT',
          'MACOMB',
          'OAKLAND',
          'WAYNE',
        ])
        group2 = set([
          'ALPENA',
          'ARENAC',
          'BAY',
          'BRANCH',
          'CALHOUN',
          'CLARE',
          'CRAWFORD',
          'GENESEE',
          'GLADWIN',
          'INGHAM',
          'ISABELLA',
          'JACKSON',
          'LAPEER',
          'LIVINGSTON',
          'LUCE',
          'MANISTEE',
          'MONROE',
          'MONTCALM',
          'MONTMORENCY',
          'ROSCOMMON',
          'SAGINAW',
          'SANILAC',
          'SHIAWASSEE',
          'ST. CLAIR',
          'TUSCOLA',
          'WASHTENAW'
        ])
        return await self.calc_humana_workaround(state_counties, sc_dict, processed_counties, params, zips, [group1, group2])
      
      for county in state_counties:
        if county not in processed_counties and county not in city_items:
            logging.info(f"Processing county: {county}")
            try:
                # Find a zip code for this county
                county_zip = next(z for z in sc_dict.get(county, []))
                logging.info(f"county_zip: {county_zip}")
                params['zip5'] = county_zip

                if county_zip is None:
                  logging.warn(f"No 1:1 zip code found for county: {county}")
                  zip_to_use = zips.lookup_zip(county)
                  params['zip5'] = zip_to_use
                  params['county'] = county
                elif 'county' in params:
                  params.pop('county')
                
                rr = await self.fetch_quote(**params)
                
                if len(rr) > 0:
                    x = rr[0]
                    county_base_raw = set(x['location_base']['county'])
                    if state == 'LA':
                      county_base_raw = process_st(county_base_raw)

                    cbr_i = list(county_base_raw)
                    for x in cbr_i:
                      if x.endswith(' CITY'):
                          city_items.add(x[:-5])
                      if state == 'FL':
                        if x == 'SAINT JOHNS':
                          city_items.add(x)
                          county_base_raw.add('ST. JOHNS')
                        if x == 'SAINT LUCIE':
                          city_items.add(x)
                          county_base_raw.add('ST. LUCIE')

                          
                    county_base = county_base_raw - city_items

                    logging.info(f"{len(county_base)} counties for {naic}")
                    
                    # Check if county_base is already in lookup_list
                    existing_index = next((i for i, s in enumerate(lookup_list) if s == county_base), None)
                    if existing_index is not None:
                        # Replace the existing set with the new one
                        county_base.add(county)
                        lookup_list[existing_index] = county_base
                    else:
                        # If not found, append the new set
                        county_base.add(county)
                        lookup_list.append(county_base)
                    
                    processed_counties.update(county_base)
                    logging.info(f"{len(processed_counties)} counties processed for {naic} -- {county}")
                    
                    if len(county_base) == 0:
                        zero_count += 1
                        if zero_count > 5:
                            logging.warn(f"{naic} has {zero_count} zero regions -- exiting")
                            return []
                else:
                    logging.warn(f"No results for {county_zip} - {county}")
            except Exception as ee:
                logging.warn(f"Error processing {county}: {ee}")

    logging.info(f"{naic} has {len(lookup_list)} county regions")

    # Check for missing counties
    missing_counties = set(state_counties) - processed_counties
    if missing_counties:
        logging.warn(f"Missing counties for NAIC {naic} in state {state}: {', '.join(sorted(missing_counties))}")
    
    # Check coverage percentage
    coverage_percentage = (len(processed_counties) / len(state_counties)) * 100
    logging.info(f"Coverage for NAIC {naic} in state {state}: {coverage_percentage:.2f}% ({len(processed_counties)}/{len(state_counties)} counties)")

    if coverage_percentage < 95:
        logging.warn(f"Low coverage ({coverage_percentage:.2f}%) for NAIC {naic} in state {state}")

    return lookup_list
  
  async def calc_naic_map_combined2(self, state, naic, effective_date = None):
     zips = zipHolder('static/uszips.csv')
     state_zips = [k for (k, v) in zips.zip_states.items() if v == state]
     random.shuffle(state_zips)

     if effective_date is None:
        effective_date = (datetime.now() + timedelta(days=32)).replace(day=1).strftime('%Y-%m-%d')

     params = {
      "zip5": state_zips[0],
      "age": 65,
      "gender": "M",
      "tobacco": 0,
      "effective_date": effective_date,
      "naic": naic,
     }

     try:
        first_result = await self.fetch_quote(**params)
        if len(first_result) == 0:
          logging.warn(f"No results for {naic} in {state}. The plan may not be offered in this state.")
          out = ([], None)
        elif len(first_result[0]['location_base']['zip5']) > 0:
          res = await self.calc_naic_map_zip(state, naic, first_result)
          out = (res, 'zip5') 
        elif len(first_result[0]['location_base']['county']) > 0:
          res = await self.calc_naic_map_county(state, naic, first_result)
          out = (res, 'county')
        else:
          logging.warn(f"This state/naic does not support combined mapping: {state}/{naic}")
          out = ([], None)
     except Exception as ee:
        logging.warn(f"EXA -- Error in initial query for {state}, {naic}: {ee}")
        out = ([], None)
     lookup_list0, mapping_type = out  
     lookup_list = sorted(lookup_list0, key = len, reverse = True)
     return lookup_list, mapping_type

  async def calc_naic_map_combined(self, state, naic):
    zips = zipHolder('static/uszips.csv')
    state_zips = [k for (k, v) in zips.zip_states.items() if v == state]
    state_counties = set()

    for z in state_zips:
        counties = zips.lookup_county(z)
        state_counties.update(counties)

    state_counties.discard('None')
    state_counties = list(state_counties)

    lookup_list = []
    processed_items = set()
    mapping_type = None

    zero_count = 0
    max_zero_count = 10
    max_error_count = 10
    error_count = 0
    consecutive_empty_results = 0
    max_consecutive_empty_results = 5

    dt = (datetime.now() + timedelta(days=32)).replace(day=1).strftime('%Y-%m-%d')

    # Shuffle zips and counties to avoid always starting with the same one
    random.shuffle(state_zips)
    random.shuffle(state_counties)


    # Try to determine mapping type
    for initial_item in state_zips + state_counties:
        try:
            if initial_item in state_zips:
                initial_county = random.choice(zips.lookup_county(initial_item))
                quote_args = {
                    "zip5": initial_item,
                    "county": initial_county,
                    "age": 65,
                    "gender": "M",
                    "tobacco": 0,
                    "effective_date": dt,
                    "naic": naic
                }
                if state not in ['MN', 'WI', 'MA', 'NY']:
                    quote_args["plan"] = "G"

                pprint(quote_args)
                rr = await self.fetch_quote(**quote_args)
            else:  # It's a county
                initial_zip = next(z for z in state_zips if initial_item in zips.lookup_county(z))
                quote_args = {
                    "zip5": initial_zip,
                    "county": initial_item,
                    "age": 65,
                    "gender": "M",
                    "tobacco": 0,
                    "effective_date": dt,
                    "naic": naic
                }
                if state not in ['MN', 'WI', 'MA', 'NY']:
                    quote_args["plan"] = "G"
                rr = await self.fetch_quote(**quote_args)
            
            if len(rr) > 0:
                consecutive_empty_results = 0  # Reset counter on successful result
                x = rr[0]
                if 'zip5' in x['location_base'] and x['location_base']['zip5']:
                    mapping_type = 'zip5'
                    items_to_process = state_zips
                elif 'county' in x['location_base'] and x['location_base']['county']:
                    mapping_type = 'county'
                    items_to_process = state_counties
                else:
                    continue  # Try next item if we can't determine mapping type

                base_items = x['location_base'][mapping_type]
                logging.info(f"{len(base_items)} {mapping_type}s for {naic}")
                lookup_list.append(base_items)
                processed_items.update(base_items)
                break  # We've successfully determined the mapping type, exit the loop
            else:
                consecutive_empty_results += 1
                if consecutive_empty_results >= max_consecutive_empty_results:
                    logging.warn(f"No results found for {naic} in {state} after {consecutive_empty_results} attempts. Aborting.")
                    return [], None
            
        except Exception as ee:
            logging.warn(f"EX -- Error in initial query for {state}, {initial_item}: {ee}")
            error_count += 1
            if error_count > max_error_count:
                logging.error(f"Max error count reached for {state}, {naic}. Aborting.")
                return [], None

    if not mapping_type:
        logging.error(f"Unable to determine mapping type for {naic} in {state}")
        return [], None

  
    city_items = []
    for item in items_to_process:
        if item not in processed_items and item not in city_items:
            print(f"Processing item: {item}")
            try:
                if mapping_type == 'zip5':
                    county = random.choice(zips.lookup_county(item))
                    if state in ['MN', 'WI', 'MA', 'NY']:
                        rr = await self.fetch_quote(zip5=item,
                                                    county=county,
                                                    age=65,
                                                    gender="M",
                                                    tobacco=0,
                                                    effective_date=dt,
                                                    naic=naic)
                    else:
                        rr = await self.fetch_quote(zip5=item,
                                                    county=county,
                                                    age=65,
                                                    gender="M",
                                                    tobacco=0,
                                                    effective_date=dt,
                                                    naic=naic,
                                                    plan="G")
                else:  # county
                    #if naic == '25178' and categorize_county(item) == 'GENERAL_GROUP' and results['GENERAL_GROUP']:
                        #continue
                    
                    county_zip = next(z for z in state_zips if item in zips.lookup_county(z))
                    if state in ['MN', 'WI', 'MA', 'NY']:
                        rr = await self.fetch_quote(zip5=county_zip,
                                                    county=item,
                                                    age=65,
                                                    gender="M",
                                                    tobacco=0,
                                                    effective_date=dt,
                                                    naic=naic)
                    else:
                        rr = await self.fetch_quote(zip5=county_zip,
                                                    county=item,
                                                    age=65,
                                                    gender="M",
                                                    tobacco=0,
                                                    effective_date=dt,
                                                    naic=naic,
                                                    plan="G")

                if len(rr) > 0:
                    consecutive_empty_results = 0  # Reset counter on successful result
                    x = rr[0]
                    base_items = x['location_base'][mapping_type]

                    if base_items:
                        if mapping_type == 'county':
                          print("base_items", base_items)
                          for x in base_items:
                              if x.endswith(' CITY'):
                                  city_name = x[:-5]  # Remove ' CITY' from the end
                                  city_items.append(city_name)
                        lookup_list.append(base_items)
                        processed_items.update(base_items)
                        logging.info(f"{len(processed_items)} {mapping_type}s processed for {naic} -- {item}")
                    else:
                        zero_count += 1
                        if zero_count > max_zero_count:
                            logging.warn(f"{naic} has {zero_count} zero regions -- exiting")
                            break
                else:
                    consecutive_empty_results += 1
                    if consecutive_empty_results >= max_consecutive_empty_results:
                        logging.warn(f"No results found for {naic} in {state} after {consecutive_empty_results} consecutive attempts. Aborting.")
                        break
                    logging.warn(f"No results for {item}")

            except Exception as ee:
                logging.warn(f"Error processing {item}: {ee}")
                error_count += 1
                if error_count > max_error_count:
                    logging.error(f"Max error count reached for {state}, {naic}. Aborting.")
                    break



    if not lookup_list:
        logging.warn(f"No data found for {naic} in {state}. The plan may not be offered in this state.")
        return [], None
    # Remove duplicate sublists from lookup_list
    unique_lookup_list = []
    seen = set()
    for sublist in lookup_list:
        # Convert the sublist to a tuple so it can be hashed
        sublist_tuple = tuple(sorted(sublist))
        if sublist_tuple not in seen:
            seen.add(sublist_tuple)
            unique_lookup_list.append(sublist)

    lookup_list = unique_lookup_list
    logging.info(f"{naic} has {len(lookup_list)} {mapping_type} regions")
    return lookup_list, mapping_type

def has_household2(xx):
  x = xx["name"]
  rating_class = xx["rating_class"]
  if rating_class:
    kk = x + ' // ' + rating_class
  else:
    kk = x
  nm = kk.lower()
  # Load name_dict from cat.csv
  name_dict = csv_to_dict('cat.csv')

  nm_list = set(
      [x['Household'].lower() for x in name_dict.values() if x['Household']])
  for x in nm_list:
    if x in nm:
      return True, kk
  return False, kk


def has_household(x):
  kk = x["fullname"]
  nm = kk.lower()
  # Load name_dict from cat.csv
  name_dict = csv_to_dict('cat.csv')

  nm_list = set(
      [x['Household'].lower() for x in name_dict.values() if x['Household']])
  for x in nm_list:
    if x in nm:
      return True
  return False


# Example usage
async def main():
  csg = AsyncCSGRequest(Config.API_KEY)
  await csg.async_init()
  # Example of making a request
  query_data = {
      'zip5': '23060',
      'gender': 'M',
      'age': 65,
      'county': 'HENRICO',
      'tobacco': 0,
      'effective_date': '2024-10-01',
      'plan': 'N',
      'naic': '25178'
  }
  response = await csg.load_response_all(query_data, delay=.2)
  return response


""" 
# Run the async main function           
r = lambda : asyncio.run(main())
import time
import statistics

from tqdm import tqdm

run_times = []
for _ in tqdm(range(20)):
    start_time = time.time()
    r()
    run_times.append(time.time() - start_time)

print("--- Min: %s seconds ---" % min(run_times))
print("--- Median: %s seconds ---" % statistics.median(run_times))
print("--- Max: %s seconds ---" % max(run_times))   
print("--- Mean: %s seconds ---" % statistics.mean(run_times))
"""

================
File: build_duckdb.py
================
#!/usr/bin/env python3
import argparse
import asyncio
import logging
import json
import sys
import itertools
import random
import os
import uuid
from typing import List, Dict, Set, Tuple, Any, Optional
from datetime import datetime, timedelta
import duckdb
from async_csg import AsyncCSGRequest as csg
from zips import zipHolder
from config import Config
from copy import copy
from functools import reduce
from filter_utils import filter_quote
import httpx
import resource
from spot_check import RateSpotChecker

# Increase system file limits if possible
try:
    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
    resource.setrlimit(resource.RLIMIT_NOFILE, (min(4096, hard), hard))
    logging.info(f"Set file descriptor limit to {min(4096, hard)}")
except Exception as e:
    logging.warning(f"Unable to increase file descriptor limit: {e}")

# Global connection pool limits
MAX_CONNECTIONS = 100
GLOBAL_SEMAPHORE = None

def setup_logging(quiet: bool) -> None:
    """Set up logging to file and console."""
    log_filename = f'build_duckdb_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler() if not quiet else logging.NullHandler()
        ]
    )

def load_carrier_selections() -> List[str]:
    """Load carrier selections from config file."""
    try:
        with open('carrier_selections.json', 'r') as f:
            data = json.load(f)
            return [naic for naic, selected in data.items() if selected]
    except FileNotFoundError:
        logging.error("carrier_selections.json not found. Please run select_carriers.py first.")
        sys.exit(1)

def generate_effective_dates(months: int = 6) -> List[str]:
    """Generate a list of effective dates (first of the next N months)."""
    today = datetime.now()
    
    # Start with the first day of next month
    if today.day == 1:
        start_date = today
    else:
        start_date = (today.replace(day=1) + timedelta(days=32)).replace(day=1)
    
    effective_dates = []
    current_date = start_date
    for _ in range(months):
        effective_dates.append(current_date.strftime('%Y-%m-%d'))
        current_date = (current_date + timedelta(days=32)).replace(day=1)
    
    return effective_dates

def get_all_states() -> List[str]:
    """Return a list of all US states and DC."""
    return [
        "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
        "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
        "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
        "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
        "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",
        "DC"
    ]


class DuckDBMedicareBuilder:
    def __init__(self, db_path: str, max_rate_limit: int = 50):
        """Initialize the DuckDB Medicare rate database builder."""
        self.db_path = db_path
        self.max_rate_limit = max_rate_limit
        
        # Connect to DuckDB
        self.conn = duckdb.connect(db_path)
        
        # Initialize API client
        self.cr = csg(Config.API_KEY)
        
        # Initialize zipHolder
        self.zip_holder = zipHolder("static/uszips.csv")
        
        # Create necessary tables and indexes
        self._create_tables()
        
        # Create shared HTTP client for connection reuse
        self.limits = httpx.Limits(max_keepalive_connections=50, max_connections=MAX_CONNECTIONS)
        
        # Initialize semaphores for concurrency control
        global GLOBAL_SEMAPHORE
        GLOBAL_SEMAPHORE = asyncio.Semaphore(MAX_CONNECTIONS)
        self.region_semaphore = asyncio.Semaphore(20)  # Limit concurrent regions
        self.demographic_semaphore = asyncio.Semaphore(30)  # Limit concurrent demographic combinations
        
        # Initialize spot checker for propagating rates across months
        self.spot_checker = RateSpotChecker(db_path)
    
    def _create_tables(self):
        """Create optimized tables and indexes for rate lookups using region-based approach."""
        # Create regions table to track unique carrier-specific regions
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS rate_regions (
                region_id TEXT,
                naic TEXT,
                state TEXT,
                mapping_type TEXT,
                region_data TEXT,  -- JSON array of ZIP codes or counties in the region
                PRIMARY KEY (region_id)
            )
        """)
        
        # Create region mapping table to map ZIP codes to regions
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS region_mapping (
                zip_code TEXT,
                region_id TEXT,
                naic TEXT,
                PRIMARY KEY (zip_code, naic)
            )
        """)
        
        # Create rate_store table at the region level
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS rate_store (
                region_id TEXT,
                gender TEXT,
                tobacco INTEGER,
                age INTEGER,
                naic TEXT,
                plan TEXT,
                rate FLOAT,
                discount_rate FLOAT,
                effective_date TEXT,
                state TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (region_id, gender, tobacco, age, naic, plan, effective_date, state)
            )
        """)
        
        # Create indexes for optimized lookups
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_region_lookup ON region_mapping (zip_code, naic)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_rate_lookup ON rate_store (region_id, gender, tobacco, age)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_naic ON rate_store (naic)")
        self.conn.execute("CREATE INDEX IF NOT EXISTS idx_effective_date ON rate_store (effective_date)")
        
        # Create carrier_info table for metadata
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS carrier_info (
                naic TEXT PRIMARY KEY,
                company_name TEXT,
                selected INTEGER DEFAULT 1
            )
        """)
        
        # Create metadata table to track processed combinations
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS processed_data (
                state TEXT,
                naic TEXT,
                effective_date TEXT,
                processed_at TIMESTAMP,
                success BOOLEAN,
                PRIMARY KEY (state, naic, effective_date)
            )
        """)
        
        # Check if api_effective_date column exists in processed_data and add it if not
        try:
            result = self.conn.execute("SELECT api_effective_date FROM processed_data LIMIT 1")
        except Exception:
            logging.info("Adding api_effective_date column to processed_data table")
            self.conn.execute("ALTER TABLE processed_data ADD COLUMN api_effective_date TEXT")
            
        # Clean up orphaned processed entries
        self.cleanup_orphaned_entries()
    
    def cleanup_orphaned_entries(self):
        """Remove entries in the processed_data table that don't have actual rate data."""
        try:
            # Find processed entries that have no corresponding rate data
            orphaned = self.conn.execute("""
                SELECT p.state, p.naic, p.effective_date
                FROM processed_data p
                WHERE p.success = true
                AND NOT EXISTS (
                    SELECT 1 FROM rate_store r 
                    WHERE r.naic = p.naic AND r.state = p.state AND r.effective_date = p.effective_date
                )
            """).fetchall()
            
            if orphaned:
                logging.info(f"Found {len(orphaned)} orphaned processed entries with no rate data")
                
                # Delete the orphaned entries
                self.conn.execute("BEGIN TRANSACTION")
                for entry in orphaned:
                    self.conn.execute("""
                        DELETE FROM processed_data
                        WHERE state = ? AND naic = ? AND effective_date = ?
                    """, entry)
                self.conn.execute("COMMIT")
                    
                logging.info(f"Cleaned up {len(orphaned)} orphaned processed entries")
                
            # Also check for orphaned region records that have no rate data
            orphaned_regions = self.conn.execute("""
                SELECT rr.region_id
                FROM rate_regions rr
                WHERE NOT EXISTS (
                    SELECT 1 FROM rate_store rs
                    WHERE rs.region_id = rr.region_id
                )
            """).fetchall()
            
            if orphaned_regions:
                logging.info(f"Found {len(orphaned_regions)} orphaned region entries with no rate data")
                
                # We won't automatically delete them, just log for manual review
                with open(f'orphaned_regions_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', 'w') as f:
                    for region in orphaned_regions:
                        f.write(f"{region[0]}\n")
                
        except Exception as e:
            logging.error(f"Error cleaning up orphaned entries: {str(e)}")
            
    def check_database_integrity(self):
        """Verify database integrity by checking relationships between tables."""
        try:
            logging.info("Checking database integrity...")
            
            # Check 1: All region_ids in rate_store exist in rate_regions
            missing_regions = self.conn.execute("""
                SELECT DISTINCT rs.region_id
                FROM rate_store rs
                LEFT JOIN rate_regions rr ON rs.region_id = rr.region_id
                WHERE rr.region_id IS NULL
            """).fetchall()
            
            if missing_regions:
                logging.error(f"Found {len(missing_regions)} region IDs in rate_store that don't exist in rate_regions")
            
            # Check 2: All region_ids in region_mapping exist in rate_regions
            missing_mapped_regions = self.conn.execute("""
                SELECT DISTINCT rm.region_id
                FROM region_mapping rm
                LEFT JOIN rate_regions rr ON rm.region_id = rr.region_id
                WHERE rr.region_id IS NULL
            """).fetchall()
            
            if missing_mapped_regions:
                logging.error(f"Found {len(missing_mapped_regions)} region IDs in region_mapping that don't exist in rate_regions")
            
            # Check 3: All NAICs in rate_store exist in carrier_info
            missing_carriers = self.conn.execute("""
                SELECT DISTINCT rs.naic
                FROM rate_store rs
                LEFT JOIN carrier_info ci ON rs.naic = ci.naic
                WHERE ci.naic IS NULL
            """).fetchall()
            
            if missing_carriers:
                logging.error(f"Found {len(missing_carriers)} NAICs in rate_store that don't exist in carrier_info")
                
            # Check 4: Count records in each table
            rate_count = self.conn.execute("SELECT COUNT(*) FROM rate_store").fetchone()[0]
            region_count = self.conn.execute("SELECT COUNT(*) FROM rate_regions").fetchone()[0]
            mapping_count = self.conn.execute("SELECT COUNT(*) FROM region_mapping").fetchone()[0]
            carrier_count = self.conn.execute("SELECT COUNT(*) FROM carrier_info").fetchone()[0]
            processed_count = self.conn.execute("SELECT COUNT(*) FROM processed_data").fetchone()[0]
            
            logging.info(f"Database contains: {rate_count} rates, {region_count} regions, " +
                        f"{mapping_count} ZIP mappings, {carrier_count} carriers, {processed_count} processed combinations")
            
            if rate_count == 0 or region_count == 0 or mapping_count == 0:
                logging.error("One or more tables are empty - database may be corrupted")
                
            # All checks passed
            if not (missing_regions or missing_mapped_regions or missing_carriers):
                logging.info("Database integrity check passed")
                return True
            else:
                logging.error("Database integrity check failed - see previous errors")
                return False
                
        except Exception as e:
            logging.error(f"Error checking database integrity: {str(e)}")
            return False
    
    async def get_available_carriers(self, state: str, effective_date: str) -> Set[str]:
        """Get all available carriers in a state with a single API call."""
        try:
            # Get a sample ZIP code for the state
            state_zips = self.zip_holder.lookup_zips_by_state(state)
            if not state_zips:
                logging.error(f"No ZIP codes found for state {state}")
                return set()
                
            sample_zip = state_zips[0]
            
            # Prepare parameters for a sample quote (without specifying NAIC)
            params = {
                "zip5": sample_zip,
                "effective_date": effective_date,
                "age": 65,
                "gender": "M",
                "tobacco": 0,
                "plan": "G"
            }
            
            # Special plan handling for certain states
            if state == 'MN':
                params['plan'] = 'MN_BASIC'
            elif state == 'WI':
                params['plan'] = 'WIR_A50%'
            elif state == 'MA':
                params['plan'] = 'MA_CORE'
            
            # Make the API call without specifying any NAIC
            response = await self.cr.fetch_quote(**params)
            
            # Extract all unique NAIC codes from the response
            available_naics = set()
            for quote in response:
                naic = quote.get('company_base', {}).get('naic')
                company_name = quote.get('company_base', {}).get('name')
                if naic:
                    available_naics.add(naic)
                    # Store carrier info
                    self.conn.execute(
                        "INSERT OR REPLACE INTO carrier_info (naic, company_name, selected) VALUES (?, ?, 1)",
                        (naic, company_name)
                    )
            
            logging.info(f"Found {len(available_naics)} available carriers in {state}: {available_naics}")
            return available_naics
            
        except Exception as e:
            logging.error(f"Error getting available carriers for state {state}: {str(e)}")
            return set()
    
    async def get_rate_regions(self, state: str, naic: str) -> Tuple[List[Set[str]], str]:
        """
        Get the geographic regions for a carrier in a state.
        Returns a list of sets (each set contains locations in a region) and the mapping type (zip5 or county).
        """
        try:
            lookup_list, mapping_type = await self.cr.calc_naic_map_combined2(state, naic)
            if len(lookup_list) == 0:
                logging.warning(f"No regions found for {naic} in {state}")
                return [], ""
            
            logging.info(f"Found {len(lookup_list)} regions for {naic} in {state} (type: {mapping_type})")
            return lookup_list, mapping_type
            
        except Exception as e:
            logging.error(f"Error getting regions for {naic} in {state}: {str(e)}")
            return [], ""
    
    def get_representative_zip(self, state: str, region: Set[str], mapping_type: str) -> str:
        """Get a representative ZIP code for a region."""
        if mapping_type == "zip5":
            # If region is ZIP-based, just pick the first ZIP
            return next(iter(region))
        else:
            # If region is county-based, get a ZIP code from each county
            for county in region:
                zips = self.zip_holder.lookup_zip_by_county(state, county)
                if zips:
                    return zips[0]
            
            # Fallback
            logging.warning(f"No ZIP found for counties {region} in {state}")
            return ""
    
    def get_all_zips_for_region(self, state: str, region: Set[str], mapping_type: str) -> List[str]:
        """Get all ZIP codes for a region."""
        if mapping_type == "zip5":
            # If region is ZIP-based, return all ZIPs in the region
            return list(region)
        else:
            # If region is county-based, get all ZIPs for each county
            all_zips = []
            for county in region:
                county_zips = self.zip_holder.lookup_zip_by_county(state, county)
                all_zips.extend(county_zips)
            return all_zips
    
    def process_quote(self, quote: Dict, region_id: str, state: str) -> List[Dict]:
        """Process a quote response into rate records."""
        filtered = filter_quote(quote)
        if not filtered:
            return []
        
        gender = filtered['gender']
        tobacco = filtered['tobacco']
        age = filtered['age']
        plan = filtered['plan']
        base_rate = filtered['rate']
        naic = filtered.get('company_base', {}).get('naic', quote.get('company_base', {}).get('naic'))
        
        # Use the effective date from the API response
        effective_date = filtered.get('effective_date', quote.get('effective_date'))
        
        # Calculate rates for different ages using age_increases
        rate_mults = [1.0] + [x + 1 for x in filtered.get('age_increases', [])]
        try:
            discount_mult = (1 - filtered.get('discounts', [{}])[0].get('value', 0))
        except:
            discount_mult = 1
            
        ages = [age + i for i in range(len(rate_mults))]
        
        results = []
        for i, current_age in enumerate(ages):
            rate_value = round(base_rate * reduce(lambda x, y: x * y, rate_mults[:i + 1]), 2)
            discount_value = round(discount_mult * rate_value, 2)
            
            results.append({
                'region_id': region_id,
                'gender': gender,
                'tobacco': int(tobacco),
                'age': current_age,
                'naic': naic,
                'plan': plan,
                'rate': rate_value,
                'discount_rate': discount_value,
                'effective_date': effective_date,
                'state': state
            })
            
        return results
    
    async def fetch_rates(self, state: str, naic: str, region_id: str, representative_zip: str, effective_date: str) -> List[Dict]:
        """Fetch rates for all demographics for a specific region."""
        try:
            # Generate all demographic combinations
            tobacco_options = [0, 1]
            gender_options = ["M", "F"]
            
            # Select plan options based on state
            if state == 'MA':
                plan_options = ['MA_CORE', 'MA_SUPP1']
            elif state == 'MN':
                plan_options = ['MN_BASIC', 'MN_EXTB']
            elif state == 'WI':
                plan_options = ['WIR_A50%']
            else:
                plan_options = ['N', 'G', 'F']
            
            # Generate combinations for key ages (65, 70, 75, etc.), then use rate multipliers for ages in between
            base_ages = [65, 70, 75, 80, 85, 90, 95]
            
            results = []
            
            # Process one plan at a time to reduce connection load
            for plan in plan_options:
                plan_tasks = []
                
                # Create tasks for parallel API calls with controlled concurrency
                for gender, tobacco, base_age in itertools.product(gender_options, tobacco_options, base_ages):
                    async def fetch_and_process(gender, tobacco, plan, age):
                        async with self.demographic_semaphore:
                            async with GLOBAL_SEMAPHORE:
                                params = {
                                    "zip5": representative_zip,
                                    "naic": naic,
                                    "gender": gender,
                                    "tobacco": tobacco,
                                    "age": age,
                                    "plan": plan,
                                    "effective_date": effective_date
                                }
                                
                                try:
                                    # Fetch quote from API
                                    response = await self.cr.fetch_quote(**params)
                                    
                                    # Process response
                                    combo_results = []
                                    for quote in response:
                                        if quote.get('company_base', {}).get('naic') == naic:
                                            processed_rates = self.process_quote(quote, region_id, state)
                                            combo_results.extend(processed_rates)
                                    
                                    return combo_results
                                except Exception as e:
                                    logging.error(f"Error fetching rates for {naic}/{gender}/{tobacco}/{plan}/{age}: {str(e)}")
                                    return []
                
                    task = asyncio.create_task(fetch_and_process(gender, tobacco, plan, base_age))
                    plan_tasks.append(task)
                
                # Wait for all tasks for this plan to complete
                for completed_task in asyncio.as_completed(plan_tasks):
                    try:
                        combo_results = await completed_task
                        results.extend(combo_results)
                    except Exception as e:
                        logging.error(f"Task completion error: {str(e)}")
            
            return results
            
        except Exception as e:
            logging.error(f"Error fetching rates for {naic} in {state} at {representative_zip}: {str(e)}")
            return []
    
    def save_rates(self, rates: List[Dict]):
        """Save rates to the database in batches, avoiding duplicates based on the primary key."""
        if not rates:
            logging.info("No rates to save.")
            return
        
        # Extract values for batch insert
        rows = [
            (r['region_id'], r['gender'], r['tobacco'], r['age'], r['naic'], 
             r['plan'], r['rate'], r['discount_rate'], r['effective_date'], r['state'])
            for r in rates
        ]
        
        batch_size = 500  # Using reasonable batch size for better performance
        inserted_count = 0
        skipped_count = 0
        error_count = 0
        correction_log_file = f'rate_corrections_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        
        # Create a temporary table to handle bulk inserts and duplicate checking
        try:
            # Create a temp table with the same structure as rate_store
            self.conn.execute("""
                CREATE TEMP TABLE IF NOT EXISTS temp_rates (
                    region_id TEXT,
                    gender TEXT,
                    tobacco INTEGER,
                    age INTEGER,
                    naic TEXT,
                    plan TEXT,
                    rate FLOAT,
                    discount_rate FLOAT,
                    effective_date TEXT,
                    state TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Process batches
            for i in range(0, len(rows), batch_size):
                batch = rows[i:i+batch_size]
                batch_inserted = 0
                batch_skipped = 0
                
                try:
                    # Begin transaction for this batch
                    self.conn.execute("BEGIN TRANSACTION")
                    
                    # Clear temporary table
                    self.conn.execute("DELETE FROM temp_rates")
                    
                    # Insert batch into temporary table
                    self.conn.executemany("""
                        INSERT INTO temp_rates 
                        (region_id, gender, tobacco, age, naic, plan, rate, discount_rate, effective_date, state)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, batch)
                    
                    # Find existing records that would conflict
                    existing_records = self.conn.execute("""
                        SELECT r.region_id, r.gender, r.tobacco, r.age, r.naic, r.plan, 
                               r.rate, r.discount_rate, r.effective_date, r.state, t.rate, t.discount_rate
                        FROM rate_store r
                        JOIN temp_rates t ON 
                            r.region_id = t.region_id AND
                            r.gender = t.gender AND
                            r.tobacco = t.tobacco AND
                            r.age = t.age AND
                            r.naic = t.naic AND
                            r.plan = t.plan AND
                            r.effective_date = t.effective_date AND
                            r.state = t.state
                    """).fetchall()
                    
                    # Log potential corrections
                    for record in existing_records:
                        old_rate, old_discount = record[6], record[7]
                        new_rate, new_discount = record[10], record[11]
                        
                        if old_rate != new_rate or old_discount != new_discount:
                            # Log rate correction
                            with open(correction_log_file, 'a') as f:
                                f.write(
                                    f"{datetime.now()}: Potential rate correction for NAIC {record[4]}, "
                                    f"Plan {record[5]}, Effective Date {record[8]}, State {record[9]}: "
                                    f"Stored Rate ${old_rate:.2f} -> New Rate ${new_rate:.2f}, "
                                    f"Stored Discount ${old_discount:.2f} -> New Discount ${new_discount:.2f}\n"
                                )
                        
                        batch_skipped += 1
                    
                    # Insert records that don't exist in rate_store using a NOT EXISTS subquery
                    inserted = self.conn.execute("""
                        INSERT INTO rate_store 
                        (region_id, gender, tobacco, age, naic, plan, rate, discount_rate, effective_date, state)
                        SELECT t.region_id, t.gender, t.tobacco, t.age, t.naic, t.plan, 
                               t.rate, t.discount_rate, t.effective_date, t.state
                        FROM temp_rates t
                        WHERE NOT EXISTS (
                            SELECT 1 FROM rate_store r
                            WHERE r.region_id = t.region_id AND
                                  r.gender = t.gender AND
                                  r.tobacco = t.tobacco AND
                                  r.age = t.age AND
                                  r.naic = t.naic AND
                                  r.plan = t.plan AND
                                  r.effective_date = t.effective_date AND
                                  r.state = t.state
                        )
                    """)
                    
                    # Get number of rows inserted
                    batch_inserted = self.conn.execute("SELECT changes()").fetchone()[0]
                    inserted_count += batch_inserted
                    skipped_count += batch_skipped
                    
                    # Commit transaction
                    self.conn.execute("COMMIT")
                    
                    logging.info(
                        f"Batch {i//batch_size + 1}/{(len(rows) + batch_size - 1) // batch_size}: "
                        f"Inserted {batch_inserted}, skipped {batch_skipped} records"
                    )
                    
                except Exception as e:
                    self.conn.execute("ROLLBACK")
                    error_count += 1
                    logging.error(f"Error in batch {i//batch_size + 1}: {str(e)}")
                    
                    # If too many errors, abort
                    if error_count > 5:
                        logging.error("Too many errors, aborting rate insertion")
                        break
            
            # Clean up temp table
            self.conn.execute("DROP TABLE IF EXISTS temp_rates")
            
        except Exception as e:
            logging.error(f"Fatal error in save_rates: {str(e)}")
            try:
                self.conn.execute("ROLLBACK")
                self.conn.execute("DROP TABLE IF EXISTS temp_rates")
            except:
                pass
            return
        
        # Verify data was saved by checking counts
        verification_count = self.conn.execute(
            """SELECT COUNT(*) FROM rate_store 
               WHERE naic = ? AND effective_date = ?""", 
            (rates[0]['naic'], rates[0]['effective_date'])
        ).fetchone()[0]
        
        if verification_count == 0:
            logging.error(f"No rates were saved for {rates[0]['naic']} on {rates[0]['effective_date']}")
        else:
            logging.info(
                f"Completed processing {len(rates)} potential rate records: "
                f"{inserted_count} inserted, {skipped_count} skipped, {verification_count} total in database"
            )
            
        if skipped_count > 0 and os.path.exists(correction_log_file):
            logging.info(f"Check {correction_log_file} for potential rate corrections.")
    
    def mark_processed(self, state: str, naic: str, effective_date: str, success: bool, api_effective_date: str = None):
        """Mark a carrier-state-date combination as processed."""
        try:
            # Check if api_effective_date column exists
            try:
                self.conn.execute("""
                    INSERT OR REPLACE INTO processed_data 
                    (state, naic, effective_date, api_effective_date, processed_at, success)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (state, naic, effective_date, api_effective_date or effective_date, datetime.now(), success))
            except Exception as col_error:
                if "api_effective_date" in str(col_error):
                    # Insert without the api_effective_date column
                    self.conn.execute("""
                        INSERT OR REPLACE INTO processed_data 
                        (state, naic, effective_date, processed_at, success)
                        VALUES (?, ?, ?, ?, ?)
                    """, (state, naic, effective_date, datetime.now(), success))
                else:
                    raise col_error
        except Exception as e:
            logging.error(f"Error marking processed: {str(e)}")
    
    def is_already_processed(self, state: str, naic: str, effective_date: str) -> bool:
        """Check if a carrier-state-date combination has already been processed successfully."""
        # First check if it's marked as processed
        result = self.conn.execute("""
            SELECT success FROM processed_data 
            WHERE state = ? AND naic = ? AND effective_date = ? AND success = true
        """, (state, naic, effective_date)).fetchone()
        
        if not result:
            return False
            
        # Also verify that we actually have rate data stored
        rate_count = self.conn.execute("""
            SELECT COUNT(*) FROM rate_store r 
            JOIN region_mapping m ON r.region_id = m.region_id AND r.naic = m.naic
            WHERE r.naic = ? AND r.state = ? AND r.effective_date = ?
            LIMIT 1
        """, (naic, state, effective_date)).fetchone()[0]
        
        # Only consider it processed if we have actual rate data
        return rate_count > 0
    
    def save_region_data(self, state: str, naic: str, regions: List[Set[str]], mapping_type: str) -> List[str]:
        """
        Save region data to database and return list of region IDs.
        This handles both creating the region entries and mapping ZIP codes to regions.
        """
        region_ids = []
        
        try:
            # Begin transaction to ensure all regions and mappings are saved together
            self.conn.execute("BEGIN TRANSACTION")
            
            for region in regions:
                # Generate a unique region ID
                region_id = str(uuid.uuid4())
                region_ids.append(region_id)
                
                # Convert region data (set of strings) to JSON
                region_data = json.dumps(list(region))
                
                # Save region metadata - using INSERT OR REPLACE to handle cases where the region already exists
                self.conn.execute("""
                    INSERT OR REPLACE INTO rate_regions 
                    (region_id, naic, state, mapping_type, region_data)
                    VALUES (?, ?, ?, ?, ?)
                """, (region_id, naic, state, mapping_type, region_data))
                
                # Get all zip codes for this region and map them to the region ID
                zip_codes = self.get_all_zips_for_region(state, region, mapping_type)
                
                # Create ZIP code to region mapping
                zip_mappings = [(zip_code, region_id, naic) for zip_code in zip_codes]
                
                # Use batch inserts with a reasonable batch size to avoid excessive memory usage
                batch_size = 500
                for i in range(0, len(zip_mappings), batch_size):
                    batch = zip_mappings[i:i+batch_size]
                    self.conn.executemany("""
                        INSERT OR REPLACE INTO region_mapping
                        (zip_code, region_id, naic)
                        VALUES (?, ?, ?)
                    """, batch)
                
                logging.info(f"Mapped {len(zip_mappings)} ZIP codes to region {region_id} for {naic} in {state}")
            
            # Verify region data was saved properly
            saved_regions_count = self.conn.execute(
                "SELECT COUNT(*) FROM rate_regions WHERE naic = ? AND state = ?", 
                (naic, state)
            ).fetchone()[0]
            
            if saved_regions_count < len(regions):
                logging.error(f"Region verification failed: Expected at least {len(regions)} regions, got {saved_regions_count}")
                self.conn.execute("ROLLBACK")
                return []
            
            # Verify mappings were saved properly (at least some mappings should exist)
            mapping_count = self.conn.execute(
                "SELECT COUNT(*) FROM region_mapping WHERE naic = ? AND region_id IN " + 
                f"({','.join(['?'] * len(region_ids))})",
                (naic, *region_ids)
            ).fetchone()[0]
            
            if mapping_count == 0:
                logging.error(f"Mapping verification failed: No mappings were created")
                self.conn.execute("ROLLBACK")
                return []
            
            # Commit the transaction only if everything was verified
            self.conn.execute("COMMIT")
            return region_ids
            
        except Exception as e:
            self.conn.execute("ROLLBACK")
            logging.error(f"Error saving region data: {str(e)}")
            return []
    
    async def process_carrier_state(self, state: str, naic: str, effective_date: str, dry_run: bool = False) -> bool:
        """Process a specific carrier-state combination for an effective date."""
        try:
            # Check if already processed
            if self.is_already_processed(state, naic, effective_date):
                logging.info(f"Skipping already processed: {state}/{naic}/{effective_date}")
                return True
                
            if dry_run:
                logging.info(f"Dry run: would process {state}/{naic}/{effective_date}")
                return True
                
            logging.info(f"Processing {state}/{naic}/{effective_date}")
            
            # Check if we have existing regions for this carrier-state combination
            existing_regions = self.conn.execute("""
                SELECT region_id, region_data, mapping_type 
                FROM rate_regions 
                WHERE naic = ? AND state = ?
            """, (naic, state)).fetchall()
            
            # If we have existing regions, reuse them instead of creating new ones
            if existing_regions:
                logging.info(f"Reusing {len(existing_regions)} existing regions for {naic} in {state}")
                region_ids = [row[0] for row in existing_regions]
                
                # Verify region mappings exist
                mapping_count = self.conn.execute(f"""
                    SELECT COUNT(*) FROM region_mapping 
                    WHERE naic = ? AND region_id IN ({','.join(['?'] * len(region_ids))})
                """, [naic] + region_ids).fetchone()[0]
                
                if mapping_count == 0:
                    logging.error(f"No region mappings found for {naic} in {state} despite existing regions")
                    return False
                
                # Use existing regions with their mapping data
                regions = []
                mapping_type = ""
                for row in existing_regions:
                    region_id, region_data_json, region_mapping_type = row
                    try:
                        region_data = set(json.loads(region_data_json))
                        regions.append(region_data)
                        mapping_type = region_mapping_type
                    except Exception as e:
                        logging.error(f"Error parsing region data: {str(e)}")
            
            else:
                # Get geographic regions for this carrier-state (only if we don't have existing ones)
                regions, mapping_type = await self.get_rate_regions(state, naic)
                if not regions:
                    logging.warning(f"No regions found for {naic} in {state}")
                    self.mark_processed(state, naic, effective_date, False)
                    return False
                
                # Save region data to database and get region IDs
                region_ids = self.save_region_data(state, naic, regions, mapping_type)
                if not region_ids:
                    logging.error(f"Failed to save region data for {naic} in {state}")
                    self.mark_processed(state, naic, effective_date, False)
                    return False
                
                # Verify region mappings exist
                mapping_count = self.conn.execute(f"""
                    SELECT COUNT(*) FROM region_mapping 
                    WHERE naic = ? AND region_id IN ({','.join(['?'] * len(region_ids))})
                """, [naic] + region_ids).fetchone()[0]
                
                if mapping_count == 0:
                    logging.error(f"No region mappings found for {naic} in {state} after save")
                    self.mark_processed(state, naic, effective_date, False)
                    return False
            
            # Process each region in batches with controlled concurrency
            all_rates = []
            batch_size = 5  # Process 5 regions at a time
            
            for i in range(0, len(regions), batch_size):
                batch_regions = regions[i:i+batch_size]
                batch_region_ids = region_ids[i:i+batch_size]
                current_tasks = []
                
                for region_index, (region, region_id) in enumerate(zip(batch_regions, batch_region_ids)):
                    # Get a representative ZIP for this region
                    rep_zip = self.get_representative_zip(state, region, mapping_type)
                    if not rep_zip:
                        logging.warning(f"Could not find representative ZIP for region {region_index+1}")
                        continue
                    
                    # Create task for fetching rates for this region
                    async with self.region_semaphore:
                        task = asyncio.create_task(self.fetch_rates(
                            state, naic, region_id, rep_zip, effective_date
                        ))
                        current_tasks.append((task, region_id, region_index))
                
                # Process fetched rates as they complete
                for task_data in current_tasks:
                    task, region_id, region_index = task_data
                    try:
                        # Wait for this region's rates
                        region_rates = await task
                        
                        if not region_rates:
                            logging.warning(f"No rates returned for {naic} in {state} for region {region_id}")
                            continue
                        
                        logging.info(f"Processing rates for region {region_index+1}/{len(regions)} with {len(region_rates)} rate records")
                        all_rates.extend(region_rates)
                        
                        # Save rates in batches to avoid memory issues
                        if len(all_rates) >= 50000:
                            self.save_rates(all_rates)
                            all_rates = []
                    
                    except Exception as region_error:
                        logging.error(f"Error processing region {region_index+1} for {naic} in {state}: {str(region_error)}")
                        continue  # Continue with other regions
                
                # Save after each batch to free memory
                if all_rates:
                    self.save_rates(all_rates)
                    all_rates = []
            
            # Save any remaining rates
            if all_rates:
                self.save_rates(all_rates)
            
            # Verify rates were actually saved
            rate_count = self.conn.execute("""
                SELECT COUNT(*) FROM rate_store 
                WHERE naic = ? AND state = ? AND effective_date = ?
            """, (naic, state, effective_date)).fetchone()[0]
            
            if rate_count == 0:
                logging.error(f"No rates were saved for {naic} in {state} on {effective_date}")
                self.mark_processed(state, naic, effective_date, False)
                return False
            
            # Mark as processed
            self.mark_processed(state, naic, effective_date, True)
            logging.info(f"Successfully processed {state}/{naic}/{effective_date} with {rate_count} rates")
            return True
            
        except Exception as e:
            logging.error(f"Error processing {state}/{naic}/{effective_date}: {str(e)}")
            self.mark_processed(state, naic, effective_date, False)
            return False
    
    async def build_database(self, states: List[str], selected_naics: List[str], effective_dates: List[str], dry_run: bool = False, spot_check: bool = True):
        """
        Build the database for specified states, carriers, and dates.
        
        Uses the natural effective dates from the API responses rather than copying rates forward.
        """
        # Initialize API client
        await self.cr.async_init()
        await self.cr.fetch_token()
        
        # Initialize spot checker
        await self.spot_checker.init()
        
        if len(effective_dates) == 0:
            logging.error("No effective dates provided")
            return
            
        # If spot checking is enabled, we'll only fully process the first month
        if spot_check and len(effective_dates) > 1:
            first_month = effective_dates[0]
            logging.info(f"Processing first month ({first_month}) fully, then checking last month for changes")
            
            # Process first month fully for all states
            await self._process_first_month(states, selected_naics, first_month, dry_run)
            
            # Now use spot checker for the final month, only adding entries if rates changed
            if not dry_run:
                # Create process function that will be called for carriers with rate changes
                async def process_carrier_with_changes(state: str, naic: str, effective_date: str) -> bool:
                    return await self.process_carrier_state(state, naic, effective_date, False)
                
                # Check for rate changes between first and last month
                await self.spot_checker.process_all_states_months(
                    states, effective_dates, process_carrier_with_changes
                )
        else:
            # Process all months fully (original behavior)
            # Process states in smaller groups to control total number of concurrent operations
            state_batch_size = 2  # Process 2 states at a time
            
            for i in range(0, len(states), state_batch_size):
                state_batch = states[i:i+state_batch_size]
                state_tasks = []
                
                # Create a task for each state in the current batch
                for state in state_batch:
                    task = asyncio.create_task(self._process_state(state, selected_naics, effective_dates, dry_run))
                    state_tasks.append((task, state))
                
                # Wait for all states in this batch to complete
                for task_data in state_tasks:
                    task, state = task_data
                    try:
                        await task
                    except Exception as e:
                        logging.error(f"Error processing state {state}: {str(e)}")
        
        logging.info("Database build completed")
    
    async def _process_first_month(self, states: List[str], selected_naics: List[str], effective_date: str, dry_run: bool = False):
        """Process just the first month fully for all states."""
        # Process states in smaller groups to control total number of concurrent operations
        state_batch_size = 2  # Process 2 states at a time
        
        for i in range(0, len(states), state_batch_size):
            state_batch = states[i:i+state_batch_size]
            state_tasks = []
            
            # Create a task for each state in the current batch
            for state in state_batch:
                task = asyncio.create_task(
                    self._process_state(state, selected_naics, [effective_date], dry_run)
                )
                state_tasks.append((task, state))
            
            # Wait for all states in this batch to complete
            for task_data in state_tasks:
                task, state = task_data
                try:
                    await task
                except Exception as e:
                    logging.error(f"Error processing state {state}: {str(e)}")
        
        logging.info(f"First month ({effective_date}) processing completed")
    
    async def _process_state(self, state: str, selected_naics: List[str], effective_dates: List[str], dry_run: bool = False):
        """Process all carriers for a single state."""
        try:
            logging.info(f"Processing state: {state}")
            
            # Get available carriers for this state
            available_carriers = await self.get_available_carriers(state, effective_dates[0])
            if not available_carriers:
                logging.warning(f"No carriers available for {state}")
                return
            
            # Filter carriers based on selected NAICs
            carriers_to_process = [n for n in selected_naics if n in available_carriers]
            logging.info(f"Processing {len(carriers_to_process)}/{len(available_carriers)} carriers in {state}")
            
            # Process carriers in batches to prevent too many open files
            carrier_batch_size = 3  # Process 3 carriers at a time
            
            for i in range(0, len(carriers_to_process), carrier_batch_size):
                carrier_batch = carriers_to_process[i:i+carrier_batch_size]
                
                # Create a semaphore to limit concurrency within this carrier batch
                batch_semaphore = asyncio.Semaphore(len(carrier_batch) * len(effective_dates))
                
                async def process_with_semaphore(state, naic, effective_date):
                    async with batch_semaphore:
                        return await self.process_carrier_state(state, naic, effective_date, dry_run)
                
                # Create tasks for this batch of carriers
                batch_tasks = []
                for naic in carrier_batch:
                    for effective_date in effective_dates:
                        task = asyncio.create_task(
                            process_with_semaphore(state, naic, effective_date)
                        )
                        batch_tasks.append((task, naic, effective_date))
                
                # Wait for all tasks in this batch to complete
                for task_data in batch_tasks:
                    task, naic, effective_date = task_data
                    try:
                        success = await task
                        if not success:
                            logging.warning(f"Failed to process {state}/{naic}/{effective_date}")
                    except Exception as task_error:
                        logging.error(f"Error in task for {state}/{naic}/{effective_date}: {str(task_error)}")
                
                # Allow some time between batches for resources to be released
                await asyncio.sleep(1)
        except Exception as e:
            logging.error(f"Error processing state {state}: {str(e)}")
    
    def get_rate_by_zip(self, zip_code: str, gender: str, tobacco: int, age: int, effective_date: str) -> List[Dict]:
        """
        Get rates for all carriers for specific demographic at a ZIP code.
        This is a convenience method for external use.
        """
        try:
            result = self.conn.execute("""
                SELECT r.naic, c.company_name, r.plan, r.rate, r.discount_rate
                FROM rate_store r
                JOIN region_mapping m ON r.region_id = m.region_id AND r.naic = m.naic
                LEFT JOIN carrier_info c ON r.naic = c.naic
                WHERE m.zip_code = ? 
                  AND r.gender = ?
                  AND r.tobacco = ?
                  AND r.age = ?
                  AND r.effective_date = ?
                ORDER BY r.naic, r.plan
            """, [zip_code, gender, tobacco, age, effective_date]).fetchall()
            
            return [
                {
                    "naic": row[0],
                    "company_name": row[1] or "Unknown",
                    "plan": row[2],
                    "rate": row[3],
                    "discount_rate": row[4]
                }
                for row in result
            ]
            
        except Exception as e:
            logging.error(f"Error getting rates for {zip_code}: {str(e)}")
            return []
    
    def optimize_database(self):
        """Run optimizations on the database."""
        logging.info("Running database optimizations...")
        
        # Vacuum the database
        self.conn.execute("VACUUM")
        
        # Run analyze to update statistics
        self.conn.execute("ANALYZE")
        
        logging.info("Database optimization complete")
    
    def close(self):
        """Close the database connection and clean up resources."""
        if hasattr(self, 'conn') and self.conn:
            try:
                logging.info("Running database optimizations...")
                # Vacuum the database
                self.conn.execute("VACUUM")
                
                # Run analyze to update statistics
                self.conn.execute("ANALYZE")
                
                logging.info("Database optimization complete")
                
                # Close DuckDB connection
                self.conn.close()
                logging.info("Database connection closed")
            except Exception as e:
                logging.error(f"Error during database close: {str(e)}")
                
        # Close spot checker connection
        if hasattr(self, 'spot_checker') and self.spot_checker:
            try:
                self.spot_checker.close()
                logging.info("Spot checker connection closed")
            except Exception as e:
                logging.error(f"Error closing spot checker: {str(e)}")
                
        # Close any HTTP clients
        if hasattr(self, 'cr') and self.cr:
            try:
                if hasattr(self.cr, 'http_client') and self.cr.http_client:
                    self.cr.http_client.close()
                    logging.info("HTTP client connections closed")
            except Exception as e:
                logging.error(f"Error closing HTTP client: {str(e)}")


class RateSpotChecker:
    def __init__(self, db_path: str):
        """Initialize the rate spot checker with database connection."""
        self.db_path = db_path
        self.conn = duckdb.connect(db_path)
        self.cr = None
        self.semaphore = None
        self.notification_file = f'rate_change_notifications_{datetime.now().strftime("%Y%m%d")}.md'
        
    async def init(self):
        """Initialize the API client and semaphore."""
        self.cr = csg(Config.API_KEY)
        await self.cr.async_init()
        await self.cr.fetch_token()
        self.semaphore = asyncio.Semaphore(20)  # Limit concurrent checks
        
        # Initialize notification file
        with open(self.notification_file, 'w') as f:
            f.write(f"# Medicare Rate Change Notifications\n\n")
            f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## Rate Changes Detected\n\n")
        
    async def check_carrier_rates(self, state: str, naic: str, previous_date: str, 
                                 current_date: str) -> bool:
        """
        Check if rates have changed between two effective dates for a carrier.
        
        Returns True if rates have changed and need to be updated.
        """
        try:
            async with self.semaphore:
                # Get a sample ZIP code for the state
                state_zips = zipHolder("static/uszips.csv").lookup_zips_by_state(state)
                if not state_zips:
                    logging.error(f"No ZIP codes found for state {state}")
                    return False
                    
                sample_zip = state_zips[0]
                
                # Sample demographic combinations to check
                # Using 65-year-old male, non-smoker as a baseline check
                demo = {
                    "zip5": sample_zip,
                    "age": 65,
                    "gender": "M", 
                    "tobacco": 0,
                    "naic": naic
                }
                
                # Select appropriate plans based on state
                if state == 'MA':
                    plans = ['MA_CORE']
                elif state == 'MN':
                    plans = ['MN_BASIC']
                elif state == 'WI':
                    plans = ['WIR_A50%']
                else:
                    plans = ['G', 'N']  # Just check common plans G and N
                
                # Compare rates between dates
                has_changes = False
                changes_details = []
                
                for plan in plans:
                    # Check previous date
                    prev_params = {**demo, "plan": plan, "effective_date": previous_date}
                    prev_response = await self.cr.fetch_quote(**prev_params)
                    
                    # Check current date
                    curr_params = {**demo, "plan": plan, "effective_date": current_date}
                    curr_response = await self.cr.fetch_quote(**curr_params)
                    
                    # Extract rates for comparison
                    prev_rates = self._extract_rates(prev_response, naic)
                    curr_rates = self._extract_rates(curr_response, naic)
                    
                    # Log details for debugging
                    logging.info(f"Rate check: {state}/{naic}/{plan} - Previous query date: {previous_date}, Current query date: {current_date}")
                    
                    # Compare rates - detect any rate change
                    if prev_rates and curr_rates:
                        # Check if rates are different (ignoring the effective dates)
                        if prev_rates[0]['rate'] != curr_rates[0]['rate']:
                            percent_change = ((curr_rates[0]['rate'] - prev_rates[0]['rate']) / prev_rates[0]['rate']) * 100 if prev_rates[0]['rate'] != 0 else float('inf')
                            logging.info(f"Rate change detected for {state}/{naic}/{plan}: {prev_rates[0]['rate']} -> {curr_rates[0]['rate']} ({percent_change:.2f}%)")
                            has_changes = True
                            
                            # Get company name for notifications
                            company_name = "Unknown"
                            if prev_response and len(prev_response) > 0:
                                company_name = prev_response[0].get('company_base', {}).get('name', "Unknown")
                            
                            # Add to changes details for notification
                            changes_details.append({
                                'state': state,
                                'naic': naic,
                                'company': company_name,
                                'plan': plan,
                                'previous_rate': prev_rates[0]['rate'],
                                'current_rate': curr_rates[0]['rate'],
                                'previous_effective_date': prev_rates[0]['effective_date'],
                                'current_effective_date': curr_rates[0]['effective_date'],
                                'percent_change': percent_change,
                                'previous_query_date': previous_date,
                                'current_query_date': current_date
                            })
                
                # Record any detected changes to the notification file
                if has_changes and changes_details:
                    self._record_notification(changes_details)
                
                return has_changes
                
        except Exception as e:
            logging.error(f"Error checking rates for {naic} in {state}: {str(e)}")
            return True  # Assume change needed if error occurs to be safe
    
    def _record_notification(self, changes_details):
        """Record rate change notifications to a file."""
        try:
            with open(self.notification_file, 'a') as f:
                for change in changes_details:
                    f.write(f"### {change['company']} ({change['naic']}) - {change['state']} - Plan {change['plan']}\n\n")
                    f.write(f"- Previous Rate: ${change['previous_rate']:.2f} (API effective: {change['previous_effective_date']})\n")
                    f.write(f"- New Rate: ${change['current_rate']:.2f} (API effective: {change['current_effective_date']})\n")
                    f.write(f"- Change: {change['percent_change']:.2f}%\n\n")
        except Exception as e:
            logging.error(f"Error recording notification: {str(e)}")
    
    def _extract_rates(self, response, naic):
        """Extract rate values and effective dates from API response for comparison."""
        rates = []
        for quote in response:
            if quote.get('company_base', {}).get('naic') == naic:
                # Use filter_quote to properly process the quote
                filtered = filter_quote(quote)
                if filtered and 'rate' in filtered and isinstance(filtered['rate'], (int, float, str)):
                    try:
                        # Include the effective date from the response
                        rate_info = {
                            'rate': float(filtered['rate']),
                            'effective_date': filtered.get('effective_date', quote.get('effective_date'))
                        }
                        rates.append(rate_info)
                    except (ValueError, TypeError):
                        logging.warning(f"Could not convert rate to float: {filtered['rate']}")
        return rates
    
    async def process_all_states_months(self, states: List[str], effective_dates: List[str], 
                                       process_function):
        """
        Process states and dates using the API's natural effective dates.
        """
        # Skip the first date - it's already been fully processed
        if len(effective_dates) <= 1:
            return
            
        # Process each state
        for state in states:
            # Get carriers that were processed for the first month
            carriers = self._get_processed_carriers(state, effective_dates[0])
            
            if not carriers:
                logging.warning(f"No carriers found for {state} in {effective_dates[0]}")
                continue
            
            # For each carrier, check the latest date to see if rates change
            # Simplify to just check the last month against the first month
            for naic in carriers:
                try:
                    # Check if rates have changed
                    has_changes = await self.check_carrier_rates(
                        state, naic, effective_dates[0], effective_dates[-1]
                    )
                    
                    if has_changes:
                        # Process this carrier for the new date
                        logging.info(f"Processing {state}/{naic}/{effective_dates[-1]} due to rate changes")
                        await process_function(state, naic, effective_dates[-1])
                    else:
                        # No changes detected - we don't need to add new rate records
                        logging.info(f"No rate changes for {state}/{naic} - skipping")
                        
                        # Mark as processed
                        try:
                            # Try with api_effective_date
                            self.conn.execute("""
                                INSERT OR REPLACE INTO processed_data 
                                (state, naic, effective_date, api_effective_date, processed_at, success)
                                VALUES (?, ?, ?, ?, ?, ?)
                            """, (state, naic, effective_dates[-1], effective_dates[-1], datetime.now(), True))
                        except Exception as col_error:
                            if "api_effective_date" in str(col_error):
                                # Insert without api_effective_date
                                self.conn.execute("""
                                    INSERT OR REPLACE INTO processed_data 
                                    (state, naic, effective_date, processed_at, success)
                                    VALUES (?, ?, ?, ?, ?)
                                """, (state, naic, effective_dates[-1], datetime.now(), True))
                            else:
                                raise col_error
                except Exception as e:
                    logging.error(f"Error checking carrier {naic} in {state}: {str(e)}")
    
    def _get_processed_carriers(self, state: str, effective_date: str) -> List[str]:
        """Get carriers that were successfully processed for a state and date."""
        result = self.conn.execute("""
            SELECT DISTINCT naic FROM processed_data 
            WHERE state = ? AND effective_date = ? AND success = true
        """, (state, effective_date)).fetchall()
        
        return [row[0] for row in result]
    
    def _is_processed(self, state: str, naic: str, effective_date: str) -> bool:
        """Check if a carrier-state-date combination has already been processed."""
        result = self.conn.execute("""
            SELECT success FROM processed_data 
            WHERE state = ? AND naic = ? AND effective_date = ? AND success = true
        """, (state, naic, effective_date)).fetchone()
        
        return result is not None
    
    def close(self):
        """Close the database connection."""
        if self.conn:
            self.conn.close()


async def main():
    parser = argparse.ArgumentParser(description="Build a Medicare Supplement Rate database with DuckDB")
    parser.add_argument("-d", "--db", type=str, default="medicare.duckdb", help="DuckDB database file path")
    parser.add_argument("-m", "--months", type=int, default=6, help="Number of months to process")
    parser.add_argument("--states", nargs="+", help="List of states to process (e.g., TX CA)")
    parser.add_argument("--naics", nargs="+", help="List of NAIC codes to process")
    parser.add_argument("-q", "--quiet", action="store_true", help="Suppress console output")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be processed without making changes")
    parser.add_argument("--max-connections", type=int, default=100, help="Maximum number of concurrent connections")
    parser.add_argument("--no-spot-check", action="store_true", help="Process all months fully instead of using spot checks")
    parser.add_argument("--lookup", action="store_true", help="Lookup mode - query rates for a specific ZIP")
    parser.add_argument("--zip", type=str, help="ZIP code to lookup rates for")
    parser.add_argument("--age", type=int, default=65, help="Age to lookup rates for")
    parser.add_argument("--gender", type=str, choices=["M", "F"], default="M", help="Gender to lookup rates for")
    parser.add_argument("--tobacco", type=int, choices=[0, 1], default=0, help="Tobacco status to lookup rates for")
    parser.add_argument("--force-reprocess", action="store_true", help="Force reprocessing even if marked as already processed")
    parser.add_argument("--check-integrity", action="store_true", help="Run database integrity check only")
    parser.add_argument("--repair-database", action="store_true", help="Attempt to repair database issues by cleaning orphaned entries")
    
    args = parser.parse_args()
    setup_logging(args.quiet)
    
    if args.dry_run:
        logging.info("DRY RUN MODE - No changes will be made to the database")
    
    if args.force_reprocess:
        logging.info("FORCE REPROCESS MODE - Will reprocess data even if already marked as processed")
    
    # Set max connections
    global MAX_CONNECTIONS
    MAX_CONNECTIONS = args.max_connections
    
    try:
        # Initialize the database builder
        builder = DuckDBMedicareBuilder(args.db)
        
        # Run database integrity check if requested
        if args.check_integrity:
            logging.info("Running database integrity check...")
            builder.check_database_integrity()
            builder.close()
            return
            
        # Clean up orphaned entries if repair is requested
        if args.repair_database:
            logging.info("Repairing database by cleaning orphaned entries...")
            builder.cleanup_orphaned_entries()
            builder.check_database_integrity()
            builder.close()
            return
            
        # Always run a basic integrity check on startup
        builder.check_database_integrity()
        
        # If force reprocess is enabled, override the is_already_processed method
        if args.force_reprocess:
            builder.is_already_processed = lambda state, naic, effective_date: False
        
        # Lookup mode - query rates for a specific ZIP
        if args.lookup:
            if not args.zip:
                logging.error("ZIP code is required for lookup mode")
                return
                
            effective_dates = generate_effective_dates(args.months)
            effective_date = effective_dates[0]  # Use first month by default
            
            rates = builder.get_rate_by_zip(
                args.zip, args.gender, args.tobacco, args.age, effective_date
            )
            
            print(f"\nRates for ZIP {args.zip}, {args.gender}, age {args.age}, tobacco {args.tobacco}, {effective_date}:")
            print("=" * 80)
            print(f"{'NAIC':<8} {'Company':<30} {'Plan':<5} {'Rate':>10} {'Discount':>10}")
            print("-" * 80)
            
            for rate in rates:
                print(f"{rate['naic']:<8} {rate['company_name'][:30]:<30} {rate['plan']:<5} "
                      f"{rate['rate']:>10.2f} {rate['discount_rate']:>10.2f}")
            
            print("=" * 80)
            print(f"Total: {len(rates)} rates found")
            
            # Close connections and return
            builder.close()
            return
        
        # Standard operation - build database
        
        # Load carriers
        selected_naics = args.naics if args.naics else load_carrier_selections()
        logging.info(f"Selected carriers: {len(selected_naics)}")
        
        # Get states to process
        states_to_process = args.states if args.states else get_all_states()
        logging.info(f"States to process: {len(states_to_process)}")
        
        # Generate effective dates
        effective_dates = generate_effective_dates(args.months)
        logging.info(f"Effective dates: {effective_dates}")
        
        # Build the database
        await builder.build_database(
            states_to_process, 
            selected_naics, 
            effective_dates, 
            args.dry_run,
            not args.no_spot_check  # Use spot checking unless --no-spot-check is specified
        )
        
        # Run final database integrity check
        builder.check_database_integrity()
        
        # Close the database
        builder.close()
        
        logging.info("Database build completed")
        
    except KeyboardInterrupt:
        logging.info("Process interrupted by user")
    except Exception as e:
        logging.error(f"Fatal error: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

================
File: filter_utils.py
================
from pydantic import BaseModel
from typing import List, Optional
import logging
class Quote(BaseModel):
    age: int
    gender: str
    plan: str
    tobacco: int
    rate: float
    discount_rate: float
    discount_category: Optional[str] = None


class QuoteInt(BaseModel):
    age: int
    gender: str
    plan: str
    tobacco: int
    rate: int
    discount_rate: int
    discount_category: Optional[str] = None
class QuoteResponse(BaseModel):
    naic: str
    group: int
    company_name: str
    quotes: List[Quote | QuoteInt]

class QuoteComparison(BaseModel):
    has_differences: bool
    db_quotes: List[QuoteResponse]
    csg_quotes: List[QuoteResponse]
    differences: Optional[List[str]] = None

def filter_quote_fields(quoteResponse):
    desired_fields = {
        'age', 'age_increases', 'company_base', 'discounts', 'discount_category', 'fees', 'gender',
        'plan', 'rate', 'rate_increases', 'rating_class', 'tobacco', 'view_type',
        'location_base'
    }
    filtered_quotes = []
    quotes, label = quoteResponse
    for quote in quotes:
        rating_class = quote.get('rating_class')
        naic = quote['company_base'].get('naic')
        if quote.get('select'):
            continue
        if quote.get('rating_class') not in [None, '', 'Standard', 'Achieve', 'Value']:
            if naic == '79413':
                if 'Standard' in rating_class and 'Household' not in rating_class:
                    pass    
                else:
                    continue
            else:
                continue

        d = {field: quote[field] for field in desired_fields}
        if len(d['location_base']['zip5']) > 0:
            d['location'] = d['location_base']['zip5']
        else:
            d['location'] = d['location_base']['county']
        if d['discount_category'] == '':
            d['discount_category'] = None
        comp = d.pop('company_base')
        d.pop('location_base')
        d['naic'] = comp.get('naic')
        d['name'] = comp.get('name')
        rate = d.pop('rate')
        d['rate'] = rate.get('month', 0) / 100
        # debug
        if d['rate'] == 214.88:
            with open('debug.log', 'a') as f:
                f.write(f"Found rate match: {d}\n")
        if d['naic'] == '60380' and d['age'] == 65 and d['plan'] == 'G' and d['gender'] == 'M' and d['tobacco'] == 0:
            with open('debug.log', 'a') as f:
                f.write(f"Found AFLAC rate: {d}\n")
        filtered_quotes.append(d)

    # Check for duplicate quotes and keep only the one with the higher rate
    unique_quotes = {}
    for quote in filtered_quotes:
        key = (quote['naic'], quote['tobacco'], quote['age'], quote['plan'], quote['gender'])
        if key in unique_quotes:
            if quote['rate'] > unique_quotes[key]['rate']:
                unique_quotes[key] = quote
        else:
            unique_quotes[key] = quote
    
    filtered_quotes = list(unique_quotes.values())
    return filtered_quotes


def use_int(quote):
    return QuoteInt(
        age=quote.age,
        gender=quote.gender,
        plan=quote.plan,
        tobacco=quote.tobacco,
        rate=int(quote.rate*100),
        discount_rate=int(quote.discount_rate*100),
        discount_category=quote.discount_category
    )

def filter_quote(quote):
    desired_fields = {
        'age', 'age_increases', 'company_base', 'discounts', 'discount_category', 'fees', 'gender',
        'plan', 'rate', 'rate_increases', 'rating_class', 'tobacco', 'view_type',
        'location_base'
    }

    rating_class = quote.get('rating_class')
    naic = quote['company_base'].get('naic')
    if quote.get('select'):
        return None
    if quote.get('rating_class') not in [None, '', 'Standard', 'Achieve', 'Value']:
        if naic == '79413':
            if 'Standard' in rating_class and 'Household' not in rating_class:
                pass    
            else:
                return None
        else:
            return None

    d = {field: quote[field] for field in desired_fields}
    if len(d['location_base']['zip5']) > 0:
        d['location'] = d['location_base']['zip5']
    else:
        d['location'] = d['location_base']['county']
    comp = d.pop('company_base')
    d.pop('location_base')
    d['naic'] = comp.get('naic')
    d['name'] = comp.get('name')
    rate = d.pop('rate')
    d['rate'] = rate.get('month', 0) / 100
    return d


def use_int(quote):
    return QuoteInt(
        age=quote.age,
        gender=quote.gender,
        plan=quote.plan,
        tobacco=quote.tobacco,
        rate=int(quote.rate*100),
        discount_rate=int(quote.discount_rate*100),
        discount_category=quote.discount_category
    )

================
File: spot_check.py
================
#!/usr/bin/env python3
import asyncio
import logging
import json
import random
import os
from filter_utils import filter_quote
from typing import List, Dict, Set, Tuple, Any, Optional
from datetime import datetime, timedelta
import duckdb
from async_csg import AsyncCSGRequest as csg
from zips import zipHolder
from config import Config
import httpx
import resource

# Global connection pool limits
MAX_CONNECTIONS = 50
GLOBAL_SEMAPHORE = asyncio.Semaphore(MAX_CONNECTIONS)

class RateSpotChecker:
    """
    Spot checks Medicare rates for changes across months and identifies
    carriers that need to be fully reprocessed for specific months.
    """
    
    def __init__(self, db_path: str):
        """Initialize the spot checker."""
        self.db_path = db_path
        self.conn = duckdb.connect(db_path)
        self.cr = csg(Config.API_KEY)
        self.zip_holder = zipHolder("static/uszips.csv")
        self.month_semaphore = asyncio.Semaphore(5)  # Limit concurrent months being processed
        
    async def init(self):
        """Initialize async components."""
        await self.cr.async_init()
        await self.cr.fetch_token()
    
    def get_carrier_list(self, state: str) -> List[str]:
        """Get list of carriers that exist in the database for a state."""
        try:
            result = self.conn.execute("""
                SELECT DISTINCT naic 
                FROM rate_store r
                JOIN rate_regions reg ON r.region_id = reg.region_id
                WHERE reg.state = ?
                ORDER BY r.naic
            """, [state]).fetchall()
            return [r[0] for r in result]
        except Exception as e:
            logging.error(f"Error getting carrier list for {state}: {str(e)}")
            return []
    
    def get_sample_demographics(self) -> List[Dict]:
        """Generate a small representative set of demographic combinations for spot checking."""
        demographics = []
        
        # Male/Female, Smoker/Non-smoker, ages 65/70/75/80
        for gender in ["M", "F"]:
            for tobacco in [0, 1]:
                for age in [65, 70, 75, 80]:
                    demographics.append({
                        "gender": gender,
                        "tobacco": tobacco,
                        "age": age
                    })
        
        # Select random subset to keep API calls manageable
        random.shuffle(demographics)
        return demographics[:6]  # Use 6 demographic combinations for spot checking
    
    def get_representative_regions(self, state: str, naic: str, limit: int = 2) -> List[Dict]:
        """Get representative regions for a carrier in a state."""
        try:
            # Select regions from the database
            result = self.conn.execute("""
                SELECT region_id, mapping_type, region_data
                FROM rate_regions
                WHERE state = ? AND naic = ?
                ORDER BY region_id
                LIMIT ?
            """, [state, naic, limit]).fetchall()
            
            if not result:
                logging.warning(f"No regions found for {naic} in {state}")
                return []
            
            regions = []
            for region_id, mapping_type, region_data in result:
                # Parse region data (JSON array of locations)
                locations = json.loads(region_data)
                
                # Get a representative ZIP code for this region
                rep_zip = None
                if mapping_type == "zip5":
                    if locations:
                        rep_zip = locations[0]
                else:  # county
                    for county in locations:
                        zips = self.zip_holder.lookup_zip_by_county(state, county)
                        if zips:
                            rep_zip = zips[0]
                            break
                
                if rep_zip:
                    regions.append({
                        "region_id": region_id,
                        "rep_zip": rep_zip,
                        "mapping_type": mapping_type
                    })
            
            return regions
            
        except Exception as e:
            logging.error(f"Error getting representative regions for {naic} in {state}: {str(e)}")
            return []
    
    async def spot_check_carrier(self, state: str, naic: str, source_date: str, 
                                target_date: str) -> bool:
        """
        Spot check if rates changed between source_date and target_date.
        Returns True if rates changed and carrier needs updating.
        """
        try:
            # Get representative regions
            regions = self.get_representative_regions(state, naic)
            if not regions:
                logging.error(f"No regions found for {naic} in {state}")
                return False
            
            # Get sample demographics
            demographics = self.get_sample_demographics()
            
            # Get plans for this state
            if state == 'MA':
                plans = ['MA_CORE']
            elif state == 'MN':
                plans = ['MN_BASIC']
            elif state == 'WI':
                plans = ['WIR_A50%']
            else:
                plans = ['G', 'N']
            
            # Check a sample of quotes across demographics and plans for each region
            async with GLOBAL_SEMAPHORE:
                rates_changed = False
                
                for region in regions:
                    region_id = region["region_id"]
                    rep_zip = region["rep_zip"]
                    
                    for demographic in demographics:
                        for plan in plans:
                            # Skip if we already know rates changed
                            if rates_changed:
                                continue
                                
                            # Check if rates differ between source and target date
                            changed = await self._compare_rates(
                                state, naic, region_id, rep_zip, demographic, plan,
                                source_date, target_date
                            )
                            
                            if changed:
                                logging.info(f"Rate change detected for {naic} in {state} between {source_date} and {target_date}")
                                rates_changed = True
                                break  # No need to check more combinations
            
            return rates_changed
            
        except Exception as e:
            logging.error(f"Error spot checking {naic} in {state}: {str(e)}")
            return False  # Assume no changes if error occurs
    
    async def _compare_rates(self, state: str, naic: str, region_id: str, rep_zip: str, 
                           demographic: Dict, plan: str, source_date: str, 
                           target_date: str) -> bool:
        """Compare rates between two dates and return True if they changed."""
        try:
            # Get rates from database for source date
            db_rate, db_effective_date = self._get_db_rate(
                region_id, demographic["gender"], demographic["tobacco"],
                demographic["age"], naic, plan, source_date
            )
            
            if db_rate is None:
                logging.warning(f"No rate in database for {naic} in {state} for region {region_id} for {source_date}")
                return True  # Assume change needed if source rate missing
            
            # Fetch rate from API for target date
            params = {
                "zip5": rep_zip,
                "naic": naic,
                "gender": demographic["gender"],
                "tobacco": demographic["tobacco"],
                "age": demographic["age"],
                "plan": plan,
                "effective_date": target_date
            }
            
            response = await self.cr.fetch_quote(**params)
            
            # Extract rate and effective date from response
            api_rates = []
            for quote in response:
                if quote.get('company_base', {}).get('naic') == naic:
                    filtered_quote = filter_quote(quote)
                    if filtered_quote and 'rate' in filtered_quote:
                        api_rates.append({
                            'rate': filtered_quote['rate'],
                            'effective_date': quote.get('effective_date')
                        })
            
            if not api_rates:
                logging.warning(f"No API rate found for {naic} in {state} at {rep_zip} for {target_date}")
                return True  # Assume change needed if API rate missing
            
            for api_rate_info in api_rates:
                api_rate = api_rate_info['rate']
                api_effective_date = api_rate_info['effective_date']
                if db_rate != api_rate or db_effective_date != api_effective_date:
                    logging.info(
                        f"Rate changed from {db_rate} to {api_rate} or effective date changed "
                        f"from {db_effective_date} to {api_effective_date} for {naic} at region {region_id}"
                    )
                    return True
            return False
            
        except Exception as e:
            logging.error(f"Error comparing rates: {str(e)}")
            return True  # Assume change needed if error occurs
    
    def _get_db_rate(self, region_id: str, gender: str, tobacco: int, 
                   age: int, naic: str, plan: str, effective_date: str) -> Tuple[Optional[float], Optional[str]]:
        """Get rate and effective date from database."""
        try:
            result = self.conn.execute("""
                SELECT rate, effective_date 
                FROM rate_store 
                WHERE region_id = ? 
                  AND gender = ? 
                  AND tobacco = ? 
                  AND age = ? 
                  AND naic = ? 
                  AND plan = ? 
                  AND effective_date <= ?
                ORDER BY effective_date DESC
                LIMIT 1
            """, [region_id, gender, tobacco, age, naic, plan, effective_date]).fetchone()
            
            return (result[0], result[1]) if result else (None, None)
            
        except Exception as e:
            logging.error(f"Database error getting rate: {str(e)}")
            return (None, None)
    
    async def process_carrier_month(self, state: str, naic: str, source_date: str, 
                                  target_date: str, process_function) -> bool:
        """
        Process a carrier for a specific month.
        If spot check shows changes, call the provided process_function.
        """
        try:
            # Check if target month already has data
            has_data = self._check_month_data(state, naic, target_date)
            
            if has_data:
                logging.info(f"{naic} in {state} already has data for {target_date}")
                return True
                
            # Spot check to see if rates changed
            rates_changed = await self.spot_check_carrier(state, naic, source_date, target_date)
            
            if rates_changed:
                # Process carrier with full rate refresh
                logging.info(f"Processing {naic} in {state} for {target_date} due to rate changes")
                success = await process_function(state, naic, target_date)
                return success
            else:
                # No changes detected - no need to update the database
                logging.info(f"No rate changes for {naic} in {state} between {source_date} and {target_date} - skipping")
                # Mark as processed in the database to avoid rechecking
                self.mark_processed(state, naic, target_date)
                return True
                
        except Exception as e:
            logging.error(f"Error processing {naic} in {state} for {target_date}: {str(e)}")
            return False
    
    def _check_month_data(self, state: str, naic: str, effective_date: str) -> bool:
        """Check if data already exists for a month."""
        try:
            result = self.conn.execute("""
                SELECT COUNT(*) 
                FROM rate_store r
                JOIN rate_regions reg ON r.region_id = reg.region_id
                WHERE reg.state = ? 
                  AND r.naic = ? 
                  AND r.effective_date = ?
                LIMIT 1
            """, [state, naic, effective_date]).fetchone()
            
            return result[0] > 0 if result else False
            
        except Exception as e:
            logging.error(f"Error checking month data: {str(e)}")
            return False
    
    def mark_processed(self, state: str, naic: str, effective_date: str, api_effective_date: str = None):
        """Mark a carrier-state-date combination as processed without adding new data."""
        try:
            # Check if api_effective_date column exists
            try:
                self.conn.execute("""
                    INSERT OR REPLACE INTO processed_data 
                    (state, naic, effective_date, api_effective_date, processed_at, success)
                    VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, ?)
                """, [state, naic, effective_date, api_effective_date or effective_date, True])
            except Exception as col_error:
                if "api_effective_date" in str(col_error):
                    # Insert without the api_effective_date column
                    self.conn.execute("""
                        INSERT OR REPLACE INTO processed_data 
                        (state, naic, effective_date, processed_at, success)
                        VALUES (?, ?, ?, CURRENT_TIMESTAMP, ?)
                    """, [state, naic, effective_date, True])
                else:
                    raise col_error
        except Exception as e:
            logging.error(f"Error marking processed: {str(e)}")
    
    async def process_state_months(self, state: str, months: List[str], 
                                 process_carrier_func) -> Dict[str, Dict[str, bool]]:
        """
        Process all carriers for a state across months.
        For each month after the first, spot check and copy forward when possible.
        """
        if not months:
            return {}
            
        # Get carriers for this state
        carriers = self.get_carrier_list(state)
        if not carriers:
            logging.warning(f"No carriers found for {state}")
            return {}
            
        results = {month: {} for month in months}
        source_date = months[0]  # First month is already fully processed
        
        # Process each subsequent month
        for i in range(1, len(months)):
            target_date = months[i]
            month_tasks = []
            
            # Create tasks for all carriers
            for naic in carriers:
                task = self.process_carrier_month(
                    state, naic, source_date, target_date, process_carrier_func
                )
                month_tasks.append((naic, task))
            
            # Wait for all carrier tasks to complete for this month
            for naic, task in month_tasks:
                try:
                    success = await task
                    results[target_date][naic] = success
                except Exception as e:
                    logging.error(f"Error in task for {naic} in {state} for {target_date}: {str(e)}")
                    results[target_date][naic] = False
            
            # Update source date for next iteration
            source_date = target_date
        
        return results
    
    async def process_all_states_months(self, states: List[str], months: List[str], 
                                      process_carrier_func) -> Dict[str, Dict[str, Dict[str, bool]]]:
        """Process all states across all months using spot checking and copy forward."""
        results = {}
        
        for state in states:
            async with self.month_semaphore:
                try:
                    logging.info(f"Processing {state} across {len(months)} months")
                    state_results = await self.process_state_months(state, months, process_carrier_func)
                    results[state] = state_results
                except Exception as e:
                    logging.error(f"Error processing {state} across months: {str(e)}")
                    results[state] = {month: {} for month in months[1:]}
        
        return results
    
    def close(self):
        """Close database connection."""
        if self.conn:
            self.conn.close()


# Command-line interface for standalone usage
async def main():
    import argparse
    from build_duckdb import get_all_states, generate_effective_dates, load_carrier_selections
    
    parser = argparse.ArgumentParser(description="Spot check Medicare rates across months")
    parser.add_argument("-d", "--db", type=str, default="medicare.duckdb", help="DuckDB database file path")
    parser.add_argument("-m", "--months", type=int, default=6, help="Number of months to process")
    parser.add_argument("--states", nargs="+", help="List of states to process (e.g., TX CA)")
    parser.add_argument("--naics", nargs="+", help="List of NAIC codes to process")
    parser.add_argument("-q", "--quiet", action="store_true", help="Suppress console output")
    
    args = parser.parse_args()
    
    # Set up logging
    log_filename = f'spot_check_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler() if not args.quiet else logging.NullHandler()
        ]
    )
    
    try:
        # Initialize spot checker
        checker = RateSpotChecker(args.db)
        await checker.init()
        
        # Get carriers to process
        selected_naics = args.naics if args.naics else load_carrier_selections()
        logging.info(f"Selected carriers: {len(selected_naics)}")
        
        # Get states to process
        states_to_process = args.states if args.states else get_all_states()
        logging.info(f"States to process: {len(states_to_process)}")
        
        # Generate effective dates
        effective_dates = generate_effective_dates(args.months)
        logging.info(f"Effective dates: {effective_dates}")
        
        # For standalone testing, we need a dummy process function
        async def dummy_process(state, naic, effective_date):
            logging.info(f"Would process {naic} in {state} for {effective_date}")
            return True
        
        # Process all states across months
        results = await checker.process_all_states_months(
            states_to_process, effective_dates, dummy_process
        )
        
        # Summarize results
        for state, state_results in results.items():
            logging.info(f"State {state} results:")
            for month, month_results in state_results.items():
                changes = sum(1 for success in month_results.values() if success)
                logging.info(f"  {month}: {changes}/{len(month_results)} carriers processed")
        
        # Close connections
        checker.close()
        logging.info("Spot check completed")
        
    except KeyboardInterrupt:
        logging.info("Process interrupted by user")
    except Exception as e:
        logging.error(f"Fatal error: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main())



================================================================
End of Codebase
================================================================
